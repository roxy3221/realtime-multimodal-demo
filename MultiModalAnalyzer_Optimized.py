#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MultiModalAnalyzer_Optimized.py - ä¼˜åŒ–çš„å¤šæ¨¡æ€åˆ†æç³»ç»Ÿ
é‡‡ç”¨äº‹ä»¶é©±åŠ¨+è§¦å‘å¼åˆ†ææ¶æ„ï¼Œå¤§å¹…æå‡æ€§èƒ½
ä¿®å¤ç‰ˆæœ¬ - è§£å†³äº†å˜é‡ä½œç”¨åŸŸå’Œä¾èµ–é—®é¢˜
"""

import json
import os
import cv2
import numpy as np
from typing import Dict, List, Optional, Tuple
import statistics
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
import subprocess
import traceback

# å°è¯•å¯¼å…¥å¯é€‰ä¾èµ–
try:
    import librosa
    HAS_LIBROSA = True
except ImportError:
    HAS_LIBROSA = False
    print("âš ï¸ librosaæœªå®‰è£…ï¼ŒéŸ³é¢‘åˆ†æåŠŸèƒ½å—é™")

try:
    from AliyunASRClient import QwenASRClient
    HAS_ASR_CLIENT = True
except ImportError:
    HAS_ASR_CLIENT = False
    print("âš ï¸ AliyunASRClientæœªæ‰¾åˆ°ï¼Œä½¿ç”¨æ¨¡æ‹ŸASR")

try:
    from FacialAnalyzer import FacialAnalyzer
    HAS_FACIAL_ANALYZER = True
except ImportError:
    HAS_FACIAL_ANALYZER = False
    print("âš ï¸ FacialAnalyzeræœªæ‰¾åˆ°ï¼Œä½¿ç”¨ç®€åŒ–äººè„¸åˆ†æ")

try:
    from ProsodyAnalyzer_Pro import ProsodyAnalyzer
    HAS_PROSODY_ANALYZER = True
except ImportError:
    HAS_PROSODY_ANALYZER = False
    print("âš ï¸ ProsodyAnalyzeræœªæ‰¾åˆ°ï¼Œä½¿ç”¨ç®€åŒ–éŸµå¾‹åˆ†æ")

try:
    from PauseDetector import PauseDetector
    HAS_PAUSE_DETECTOR = True
except ImportError:
    HAS_PAUSE_DETECTOR = False
    print("âš ï¸ PauseDetectoræœªæ‰¾åˆ°ï¼Œä½¿ç”¨ç®€åŒ–VAD")

try:
    from EventDrivenDetector import LightweightDetector, EventDrivenCache
    HAS_EVENT_DETECTOR = True
except ImportError:
    HAS_EVENT_DETECTOR = False
    print("âš ï¸ EventDrivenDetectoræœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤å®ç°")

try:
    from TimeAlignment import snap_to_silence
    HAS_TIME_ALIGNMENT = True
except ImportError:
    HAS_TIME_ALIGNMENT = False
    print("âš ï¸ TimeAlignmentæœªæ‰¾åˆ°ï¼Œè·³è¿‡æ—¶é—´å¯¹é½")

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# =============================================================================
# é»˜è®¤å®ç°ç±»ï¼ˆå½“ä¾èµ–ä¸å¯ç”¨æ—¶ä½¿ç”¨ï¼‰
# =============================================================================

class MockASRClient:
    """æ¨¡æ‹ŸASRå®¢æˆ·ç«¯"""
    def transcribe_and_segment(self, audio_path: str) -> List[Dict]:
        logger.info("ğŸ¤– ä½¿ç”¨æ¨¡æ‹ŸASRåˆ†æ®µ")
        # æ¨¡æ‹Ÿåˆ†æ®µï¼šå‡è®¾æ¯3ç§’ä¸€ä¸ªæ®µè½
        try:
            if HAS_LIBROSA:
                y, sr = librosa.load(audio_path, sr=None)
                duration = len(y) / sr
            else:
                # ä½¿ç”¨ffprobeè·å–æ—¶é•¿
                duration = self._get_duration_ffprobe(audio_path)
        except:
            duration = 30.0  # é»˜è®¤30ç§’
        
        segments = []
        for i in range(0, int(duration), 3):
            segments.append({
                "text": f"æ¨¡æ‹Ÿè¯­éŸ³æ®µè½{len(segments)+1}",
                "start_ms": i * 1000,
                "end_ms": min((i + 3) * 1000, int(duration * 1000)),
                "source": "MOCK_ASR",
                "punct": "ã€‚"
            })
        
        return segments
    
    def _get_duration_ffprobe(self, audio_path: str) -> float:
        """ä½¿ç”¨ffprobeè·å–éŸ³é¢‘æ—¶é•¿"""
        try:
            cmd = [
                'ffprobe', '-i', audio_path, '-show_entries', 
                'format=duration', '-v', 'quiet', '-of', 'csv=p=0'
            ]
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            return float(result.stdout.strip())
        except:
            return 30.0

class MockFacialAnalyzer:
    """æ¨¡æ‹Ÿé¢éƒ¨åˆ†æå™¨"""
    @staticmethod
    def analyze_segment(video_path: str, start_ms: int, end_ms: int) -> Dict:
        logger.debug(f"ğŸ­ æ¨¡æ‹Ÿé¢éƒ¨åˆ†æ {start_ms}-{end_ms}ms")
        return {
            "Smile": [0.5, 0.1],
            "Mouth": [0.3, 0.1],
            "EAR": [0.25, 0.05],
            "Brow": [0.1, 0.02],
            "Yaw": [0.0, 2.0],
            "Pitch": [0.0, 2.0],
            "Roll": [0.0, 1.0],
            "FaceSize": [0.2, 0.05]
        }

class MockProsodyAnalyzer:
    """æ¨¡æ‹ŸéŸµå¾‹åˆ†æå™¨"""
    @staticmethod
    def analyze_segment(audio_path: str, start_ms: int, end_ms: int, n_points: int = 15) -> Dict:
        logger.debug(f"ğŸµ æ¨¡æ‹ŸéŸµå¾‹åˆ†æ {start_ms}-{end_ms}ms")
        # ç”Ÿæˆæ¨¡æ‹Ÿçš„éŸµå¾‹æ•°æ®
        duration_s = (end_ms - start_ms) / 1000.0
        time_points = np.linspace(0, duration_s, n_points)
        
        # æ¨¡æ‹ŸåŸºé¢‘å˜åŒ–
        pitch = 200 + 50 * np.sin(2 * np.pi * time_points / duration_s * 2)
        # æ¨¡æ‹Ÿè¯­é€Ÿå˜åŒ–
        rate = 1.0 + 0.2 * np.sin(2 * np.pi * time_points / duration_s * 3)
        # æ¨¡æ‹ŸéŸ³é‡å˜åŒ–
        level = 0.7 + 0.2 * np.sin(2 * np.pi * time_points / duration_s * 1.5)
        
        return {
            "pitch": pitch.tolist(),
            "rate": rate.tolist(),
            "level": level.tolist()
        }

class MockPauseDetector:
    """æ¨¡æ‹Ÿæš‚åœæ£€æµ‹å™¨"""
    def detect_pauses_from_file(self, audio_path: str) -> Dict:
        logger.info("â¸ï¸ æ¨¡æ‹ŸVADæ£€æµ‹")
        try:
            if HAS_LIBROSA:
                y, sr = librosa.load(audio_path, sr=None)
                duration = len(y) / sr
            else:
                duration = self._get_duration_ffprobe(audio_path)
        except:
            duration = 30.0
        
        # æ¨¡æ‹Ÿè¯­éŸ³æ®µå’Œæš‚åœæ®µ
        speech_segments = []
        pause_segments = []
        
        current_time = 0
        while current_time < duration:
            # è¯­éŸ³æ®µï¼š2-4ç§’
            speech_duration = np.random.uniform(2.0, 4.0)
            speech_end = min(current_time + speech_duration, duration)
            speech_segments.append({
                'start_time': current_time,
                'end_time': speech_end
            })
            current_time = speech_end
            
            # æš‚åœæ®µï¼š0.3-0.8ç§’
            if current_time < duration:
                pause_duration = np.random.uniform(0.3, 0.8)
                pause_end = min(current_time + pause_duration, duration)
                pause_segments.append({
                    'start_time': current_time,
                    'end_time': pause_end
                })
                current_time = pause_end
        
        return {
            'speech_segments': speech_segments,
            'pause_segments': pause_segments
        }
    
    def _get_duration_ffprobe(self, audio_path: str) -> float:
        """ä½¿ç”¨ffprobeè·å–éŸ³é¢‘æ—¶é•¿"""
        try:
            cmd = [
                'ffprobe', '-i', audio_path, '-show_entries', 
                'format=duration', '-v', 'quiet', '-of', 'csv=p=0'
            ]
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            return float(result.stdout.strip())
        except:
            return 30.0

class LightweightDetector:
    """è½»é‡çº§äº‹ä»¶æ£€æµ‹å™¨çš„é»˜è®¤å®ç°"""
    
    def precompute_audio_features(self, audio_path: str) -> Dict:
        """é¢„è®¡ç®—éŸ³é¢‘ç‰¹å¾"""
        logger.info("ğŸ¯ é¢„è®¡ç®—éŸ³é¢‘ç‰¹å¾ï¼ˆè½»é‡ç‰ˆï¼‰")
        return {
            "features": "lightweight_computed",
            "timestamp": time.time()
        }
    
    def detect_audio_events(self, precomputed_audio: Dict, start_s: float, end_s: float) -> List[Dict]:
        """æ£€æµ‹éŸ³é¢‘äº‹ä»¶"""
        # ç®€å•çš„éšæœºäº‹ä»¶ç”Ÿæˆ
        events = []
        if np.random.random() > 0.7:  # 30%æ¦‚ç‡æœ‰äº‹ä»¶
            events.append({
                "type": "audio_change",
                "timestamp": (start_s + end_s) / 2,
                "confidence": 0.8
            })
        return events
    
    def detect_video_events(self, video_path: str, start_frame: int, end_frame: int) -> List[Dict]:
        """æ£€æµ‹è§†é¢‘äº‹ä»¶"""
        events = []
        if np.random.random() > 0.6:  # 40%æ¦‚ç‡æœ‰äº‹ä»¶
            events.append({
                "type": "visual_change",
                "frame": (start_frame + end_frame) // 2,
                "confidence": 0.7
            })
        return events
    
    def should_trigger_analysis(self, events: List[Dict], timestamp: float) -> Dict:
        """åˆ¤æ–­æ˜¯å¦è§¦å‘åˆ†æ"""
        # ç®€å•ç­–ç•¥ï¼šæœ‰äº‹ä»¶å°±è§¦å‘
        should_trigger = len(events) > 0 or np.random.random() > 0.5  # 50%æ¦‚ç‡è§¦å‘
        
        reasons = []
        if len(events) > 0:
            reasons.append(f"detected_{len(events)}_events")
        if should_trigger and not reasons:
            reasons.append("random_trigger")
        
        return {
            "should_trigger": should_trigger,
            "reasons": reasons
        }

class EventDrivenCache:
    """äº‹ä»¶é©±åŠ¨ç¼“å­˜çš„é»˜è®¤å®ç°"""
    
    def __init__(self):
        self.audio_cache = {}
        self.video_cache = {}
        logger.info("ğŸ’¾ åˆå§‹åŒ–äº‹ä»¶é©±åŠ¨ç¼“å­˜")
    
    def get_interpolated_audio(self, timestamp: float, n_points: int) -> Optional[Dict]:
        """è·å–æ’å€¼éŸ³é¢‘ç‰¹å¾"""
        # ç®€å•çš„æœ€è¿‘é‚»æ’å€¼
        if not self.audio_cache:
            return None
        
        closest_time = min(self.audio_cache.keys(), key=lambda t: abs(t - timestamp))
        if abs(closest_time - timestamp) < 2.0:  # 2ç§’å†…çš„ç¼“å­˜æœ‰æ•ˆ
            cached_features = self.audio_cache[closest_time].copy()
            logger.debug(f"ğŸ“‹ ä½¿ç”¨ç¼“å­˜éŸ³é¢‘ç‰¹å¾ {closest_time:.2f}s -> {timestamp:.2f}s")
            return cached_features
        
        return None
    
    def get_interpolated_video(self, timestamp: float) -> Optional[Dict]:
        """è·å–æ’å€¼è§†é¢‘ç‰¹å¾"""
        if not self.video_cache:
            return None
        
        closest_time = min(self.video_cache.keys(), key=lambda t: abs(t - timestamp))
        if abs(closest_time - timestamp) < 2.0:  # 2ç§’å†…çš„ç¼“å­˜æœ‰æ•ˆ
            cached_features = self.video_cache[closest_time].copy()
            logger.debug(f"ğŸ“‹ ä½¿ç”¨ç¼“å­˜è§†é¢‘ç‰¹å¾ {closest_time:.2f}s -> {timestamp:.2f}s")
            return cached_features
        
        return None
    
    def store_audio_features(self, timestamp: float, features: Dict):
        """å­˜å‚¨éŸ³é¢‘ç‰¹å¾"""
        if features and "sound" in features:
            self.audio_cache[timestamp] = features["sound"]
            logger.debug(f"ğŸ’¾ ç¼“å­˜éŸ³é¢‘ç‰¹å¾ {timestamp:.2f}s")
    
    def store_video_features(self, timestamp: float, features: Dict):
        """å­˜å‚¨è§†é¢‘ç‰¹å¾"""
        if features and "face" in features:
            self.video_cache[timestamp] = features["face"]
            logger.debug(f"ğŸ’¾ ç¼“å­˜è§†é¢‘ç‰¹å¾ {timestamp:.2f}s")
    
    def cleanup_old_cache(self, current_time: float, max_age: float):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        cutoff_time = current_time - max_age
        
        # æ¸…ç†éŸ³é¢‘ç¼“å­˜
        old_audio_count = len(self.audio_cache)
        self.audio_cache = {t: f for t, f in self.audio_cache.items() if t >= cutoff_time}
        
        # æ¸…ç†è§†é¢‘ç¼“å­˜
        old_video_count = len(self.video_cache)
        self.video_cache = {t: f for t, f in self.video_cache.items() if t >= cutoff_time}
        
        cleaned_audio = old_audio_count - len(self.audio_cache)
        cleaned_video = old_video_count - len(self.video_cache)
        
        if cleaned_audio > 0 or cleaned_video > 0:
            logger.info(f"ğŸ§¹ æ¸…ç†ç¼“å­˜: éŸ³é¢‘{cleaned_audio}ä¸ª, è§†é¢‘{cleaned_video}ä¸ª")

# =============================================================================
# ä¸»åˆ†æå™¨ç±»
# =============================================================================

class MultiModalAnalyzer_Optimized:
    """ä¼˜åŒ–çš„å¤šæ¨¡æ€åˆ†æç³»ç»Ÿ - äº‹ä»¶é©±åŠ¨æ¶æ„ï¼ˆå®Œæ•´ä¿®å¤ç‰ˆï¼‰"""
    
    def __init__(self, api_key: str = None, enable_segmentation: bool = True, 
                 segmentation_mode: str = "ASR_PUNCT_FIRST"):
        """
        åˆå§‹åŒ–ä¼˜åŒ–çš„å¤šæ¨¡æ€åˆ†æå™¨
        
        Args:
            api_key: é˜¿é‡Œäº‘ASR APIå¯†é’¥
            enable_segmentation: æ˜¯å¦å¯ç”¨åˆ†æ®µæ¨¡å¼
            segmentation_mode: åˆ†æ®µæ¨¡å¼
        """
        logger.info("ğŸš€ åˆå§‹åŒ–ä¼˜åŒ–çš„å¤šæ¨¡æ€åˆ†æå™¨...")
        
        # æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–ï¼ˆä½¿ç”¨å¯ç”¨çš„å®ç°ï¼‰
        if HAS_ASR_CLIENT and api_key:
            self.asr_client = QwenASRClient()
        else:
            self.asr_client = MockASRClient()
            logger.info("ä½¿ç”¨æ¨¡æ‹ŸASRå®¢æˆ·ç«¯")
        
        if HAS_FACIAL_ANALYZER:
            self.facial_analyzer = FacialAnalyzer()
        else:
            self.facial_analyzer = MockFacialAnalyzer()
            logger.info("ä½¿ç”¨æ¨¡æ‹Ÿé¢éƒ¨åˆ†æå™¨")
        
        if HAS_PROSODY_ANALYZER:
            self.prosody_analyzer = ProsodyAnalyzer()
        else:
            self.prosody_analyzer = MockProsodyAnalyzer()
            logger.info("ä½¿ç”¨æ¨¡æ‹ŸéŸµå¾‹åˆ†æå™¨")
        
        if HAS_PAUSE_DETECTOR and enable_segmentation:
            self.pause_detector = PauseDetector()
        else:
            self.pause_detector = MockPauseDetector() if enable_segmentation else None
            if enable_segmentation:
                logger.info("ä½¿ç”¨æ¨¡æ‹Ÿæš‚åœæ£€æµ‹å™¨")
        
        # ä¼˜åŒ–ç»„ä»¶
        if HAS_EVENT_DETECTOR:
            self.detector = LightweightDetector()
            self.cache = EventDrivenCache()
        else:
            self.detector = LightweightDetector()
            self.cache = EventDrivenCache()
            logger.info("ä½¿ç”¨é»˜è®¤äº‹ä»¶æ£€æµ‹å’Œç¼“å­˜")
        
        self.enable_segmentation = enable_segmentation
        self.segmentation_mode = segmentation_mode
        
        # åˆå§‹åŒ–äººè„¸çº§è”åˆ†ç±»å™¨ï¼ˆé¿å…é‡å¤åˆ›å»ºï¼‰
        try:
            self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
            if self.face_cascade.empty():
                raise ValueError("äººè„¸çº§è”åˆ†ç±»å™¨åŠ è½½å¤±è´¥")
        except Exception as e:
            logger.warning(f"äººè„¸æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.face_cascade = None
        
        # ä¼˜åŒ–é…ç½®
        self.CFG = {
            "max_segment_s": 12.0,
            "max_chars": 30,
            "vad_pause_cut_s": 0.4,
            "snap_tolerance_ms": 120,
            "prosody_points": 15,
            
            # äº‹ä»¶é©±åŠ¨é…ç½®
            "analysis_window_s": 2.0,           # åˆ†æçª—å£æ—¶é•¿
            "trigger_threshold": 0.5,           # è§¦å‘é˜ˆå€¼
            "cache_max_age_s": 300.0,           # ç¼“å­˜æœ€å¤§å¹´é¾„
            "min_recompute_interval_s": 0.5,    # æœ€å°é‡è®¡ç®—é—´éš”
            "parallel_precompute": True,        # å¹¶è¡Œé¢„è®¡ç®—
        }
        
        # é¢„è®¡ç®—ç¼“å­˜
        self._precomputed_audio = None
        self._precomputed_face_cache = {}
        self._precomputed_vad = None
        
        logger.info("âœ… å¤šæ¨¡æ€åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
        
    def analyze_video_audio(self, video_path: str, audio_path: str = None, 
                           extract_audio: bool = True) -> Dict:
        """
        ä¼˜åŒ–çš„å¤šæ¨¡æ€åˆ†æä¸»æµç¨‹
        é‡‡ç”¨: P0é¢„è®¡ç®— + P1ASRå¹¶è¡Œ + P2äº‹ä»¶é©±åŠ¨ + P3ç¼“å­˜èšåˆ
        """
        logger.info("ğŸš€ å¯åŠ¨ä¼˜åŒ–çš„å¤šæ¨¡æ€åˆ†æ...")
        start_time = time.time()
        
        try:
            # éªŒè¯æ–‡ä»¶
            if not os.path.exists(video_path):
                raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
            
            # å‡†å¤‡éŸ³é¢‘è·¯å¾„
            if audio_path is None and extract_audio:
                audio_path = self._extract_audio(video_path)
            elif audio_path is None:
                audio_path = video_path
                
            # **é˜¶æ®µP0: å¹¶è¡Œé¢„è®¡ç®—**
            logger.info("ğŸ“Š P0: å¯åŠ¨å¹¶è¡Œé¢„è®¡ç®—...")
            precompute_start = time.time()
            
            if self.CFG["parallel_precompute"]:
                # å¹¶è¡Œæ‰§è¡Œé¢„è®¡ç®—å’ŒASR
                with ThreadPoolExecutor(max_workers=3) as executor:
                    # æäº¤é¢„è®¡ç®—ä»»åŠ¡
                    audio_future = executor.submit(self._precompute_audio, audio_path)
                    video_future = executor.submit(self._precompute_video, video_path)
                    asr_future = executor.submit(self._run_asr_segmentation, audio_path)
                    
                    # ç­‰å¾…é¢„è®¡ç®—å®Œæˆ
                    precomputed_audio = audio_future.result()
                    precomputed_video = video_future.result() 
                    asr_segments = asr_future.result()
            else:
                # ä¸²è¡Œæ‰§è¡Œï¼ˆå›é€€æ¨¡å¼ï¼‰
                precomputed_audio = self._precompute_audio(audio_path)
                precomputed_video = self._precompute_video(video_path)
                asr_segments = self._run_asr_segmentation(audio_path)
                
            precompute_time = time.time() - precompute_start
            logger.info(f"âœ… P0é¢„è®¡ç®—å®Œæˆï¼Œè€—æ—¶: {precompute_time:.2f}s")
            
            # **é˜¶æ®µP1+P2: äº‹ä»¶é©±åŠ¨çš„æ®µè½å¤„ç†**
            logger.info("ğŸ¯ P1+P2: äº‹ä»¶é©±åŠ¨åˆ†æ®µå¤„ç†...")
            segments = self._process_segments_event_driven(
                asr_segments, video_path, audio_path, 
                precomputed_audio, precomputed_video
            )
            
            # **é˜¶æ®µP3+P4: æ±‡æ€»è¾“å‡º**
            total_time = time.time() - start_time
            
            result = {
                "segments": segments,
                "summary": {
                    "total_duration": precomputed_audio.get("duration", 0),
                    "segment_count": len(segments),
                    "asr_segments": len([s for s in segments if s.get("meta", {}).get("source") == "ASR_PUNCT"]),
                    "vad_fallback_segments": len([s for s in segments if s.get("meta", {}).get("source") == "VAD_FALLBACK"]),
                    "segmentation_mode": self.segmentation_mode,
                    "performance": {
                        "total_time_s": total_time,
                        "precompute_time_s": precompute_time,
                        "processing_time_s": total_time - precompute_time,
                        "speedup_ratio": f"{precomputed_audio.get('duration', 1) / total_time:.2f}x"
                    }
                }
            }
            
            logger.info(f"ğŸ‰ ä¼˜åŒ–åˆ†æå®Œæˆ! æ€»è€—æ—¶: {total_time:.2f}s, åŠ é€Ÿæ¯”: {result['summary']['performance']['speedup_ratio']}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ å¤šæ¨¡æ€åˆ†æå¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            # è¿”å›é”™è¯¯ç»“æœ
            return {
                "segments": [],
                "summary": {
                    "total_duration": 0,
                    "segment_count": 0,
                    "error": str(e),
                    "performance": {
                        "total_time_s": time.time() - start_time,
                        "status": "failed"
                    }
                }
            }
    
    def _precompute_audio(self, audio_path: str) -> Dict:
        """P0: é¢„è®¡ç®—éŸ³é¢‘ç‰¹å¾ï¼ˆä¿®å¤ç‰ˆï¼‰"""
        logger.info("ğŸ”Š é¢„è®¡ç®—éŸ³é¢‘ç‰¹å¾...")
        
        try:
            # ä½¿ç”¨è½»é‡çº§æ¢æµ‹å™¨é¢„è®¡ç®—åŸºç¡€ç‰¹å¾
            audio_features = self.detector.precompute_audio_features(audio_path)
            
            # é¢„è®¡ç®—VADä¿¡æ¯
            if self.pause_detector:
                vad_info = self._precompute_vad(audio_path)
                audio_features["vad"] = vad_info
                
            # è·å–æ—¶é•¿ï¼ˆé¿å…é‡å¤åŠ è½½éŸ³é¢‘ï¼‰
            duration = self._get_audio_duration(audio_path)
            audio_features["duration"] = duration
            
            # å¦‚æœéœ€è¦ç¼“å­˜åŸå§‹éŸ³é¢‘
            if HAS_LIBROSA:
                y, sr = librosa.load(audio_path, sr=None)
                audio_features["raw_audio"] = (y, sr)
            
            self._precomputed_audio = audio_features
            return audio_features
            
        except Exception as e:
            logger.error(f"éŸ³é¢‘é¢„è®¡ç®—å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤éŸ³é¢‘ç‰¹å¾
            return {
                "duration": 30.0,
                "features": {},
                "error": str(e)
            }
    
    def _precompute_video(self, video_path: str) -> Dict:
        """P0: é¢„è®¡ç®—è§†é¢‘ç‰¹å¾ï¼ˆç¨€ç–é‡‡æ ·ï¼Œä¿®å¤ç‰ˆï¼‰"""
        logger.info("ğŸ“¹ é¢„è®¡ç®—è§†é¢‘ç‰¹å¾...")
        
        cap = None
        try:
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
                
            fps = cap.get(cv2.CAP_PROP_FPS)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            duration = total_frames / fps if fps > 0 else 0
            
            # ç¨€ç–é‡‡æ ·ï¼šæ¯1ç§’é‡‡æ ·ä¸€å¸§ï¼ˆæé«˜æ•ˆç‡ï¼‰
            sample_interval = max(int(fps * 1.0), 1) if fps > 0 else 30
            face_cache = {}
            
            for frame_idx in range(0, total_frames, sample_interval):
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()
                if not ret:
                    break
                    
                timestamp = frame_idx / fps if fps > 0 else frame_idx / 30
                
                # å¿«é€Ÿäººè„¸æ£€æµ‹+ç‰¹å¾æå–
                face_features = self._extract_face_features_fast(frame)
                if face_features is not None:
                    face_cache[timestamp] = face_features
                    
            video_features = {
                "face_cache": face_cache,
                "fps": fps,
                "total_frames": total_frames,
                "duration": duration
            }
            
            self._precomputed_face_cache = face_cache
            logger.info(f"âœ… é¢„è®¡ç®—è§†é¢‘ç‰¹å¾å®Œæˆï¼Œé‡‡æ ·{len(face_cache)}ä¸ªå…³é”®å¸§")
            return video_features
            
        except Exception as e:
            logger.error(f"è§†é¢‘é¢„è®¡ç®—å¤±è´¥: {e}")
            return {
                "face_cache": {},
                "fps": 30,
                "total_frames": 0,
                "duration": 0,
                "error": str(e)
            }
        finally:
            if cap is not None:
                cap.release()
    
    def _precompute_vad(self, audio_path: str) -> Dict:
        """P0: é¢„è®¡ç®—VADä¿¡æ¯"""
        logger.info("ğŸ™ï¸ é¢„è®¡ç®—VADä¿¡æ¯...")
        
        try:
            result = self.pause_detector.detect_pauses_from_file(audio_path)
            speech_segments = result['speech_segments']
            pause_segments = result['pause_segments']
            
            vad_info = {
                "speech_segments": speech_segments,
                "pause_segments": pause_segments,
                "timeline": self._build_vad_timeline(speech_segments, pause_segments)
            }
            
            self._precomputed_vad = vad_info
            return vad_info
            
        except Exception as e:
            logger.error(f"VADé¢„è®¡ç®—å¤±è´¥: {e}")
            return {
                "speech_segments": [],
                "pause_segments": [],
                "timeline": {}
            }
    
    def _run_asr_segmentation(self, audio_path: str) -> List[Dict]:
        """P1: ASRåˆ†æ®µï¼ˆå¯å¹¶è¡Œæ‰§è¡Œï¼‰"""
        logger.info("ğŸ—£ï¸ P1: æ‰§è¡ŒASRåˆ†æ®µ...")
        
        try:
            segments = self.asr_client.transcribe_and_segment(audio_path)
            logger.info(f"âœ… ASRåˆ†æ®µå®Œæˆï¼Œè·å¾—{len(segments)}ä¸ªåˆå§‹æ®µè½")
            return segments
        except Exception as e:
            logger.warning(f"ASRåˆ†æ®µå¤±è´¥: {e}ï¼Œä½¿ç”¨VADå…œåº•")
            return self._fallback_vad_segmentation(audio_path)
    
    def _process_segments_event_driven(self, asr_segments: List[Dict], video_path: str, 
                                     audio_path: str, precomputed_audio: Dict, 
                                     precomputed_video: Dict) -> List[Dict]:
        """P2: äº‹ä»¶é©±åŠ¨çš„æ®µè½å¤„ç†ï¼ˆå®Œå…¨ä¿®å¤ç‰ˆï¼‰"""
        logger.info("ğŸ¯ å¼€å§‹äº‹ä»¶é©±åŠ¨å¤„ç†...")
        
        processed_segments = []
        analysis_window = self.CFG["analysis_window_s"]
        
        # ğŸ”§ ä¿®å¤1: åˆå§‹åŒ– total_end_sï¼Œé¿å… UnboundLocalError
        total_end_s = 0.0
        
        # ğŸ”§ ä¿®å¤2: å¤„ç†ç©ºæ®µè½åˆ—è¡¨çš„æƒ…å†µ
        if not asr_segments:
            logger.warning("âš ï¸ ASRæ®µè½åˆ—è¡¨ä¸ºç©ºï¼Œä½¿ç”¨éŸ³é¢‘æ€»æ—¶é•¿")
            total_end_s = precomputed_audio.get("duration", 0.0)
            self.cache.cleanup_old_cache(total_end_s, self.CFG["cache_max_age_s"])
            return processed_segments
        
        try:
            for seg_idx, asr_seg in enumerate(asr_segments):
                start_ms = asr_seg.get("start_ms", 0)
                end_ms = asr_seg.get("end_ms", start_ms + 3000)
                start_s = start_ms / 1000.0
                end_s = end_ms / 1000.0
                
                # ğŸ”§ ä¿®å¤3: è·Ÿè¸ªæœ€å¤§ç»“æŸæ—¶é—´
                total_end_s = max(total_end_s, end_s)
                
                logger.info(f"å¤„ç†æ®µè½ {seg_idx+1}/{len(asr_segments)}: {start_s:.2f}-{end_s:.2f}s")
                
                try:
                    # æ£€æŸ¥æ˜¯å¦è¶…é•¿éœ€è¦VADç»†åˆ†
                    if ((end_s - start_s) > self.CFG["max_segment_s"] or 
                        len(asr_seg.get("text", "")) > self.CFG["max_chars"]):
                        sub_segments = self._vad_split_segment_cached(start_s, end_s, asr_seg)
                    else:
                        sub_segments = [asr_seg]
                    
                    # å¤„ç†æ¯ä¸ªå­æ®µè½
                    for sub_seg in sub_segments:
                        sub_start_s = sub_seg.get("start_ms", 0) / 1000.0
                        sub_end_s = sub_seg.get("end_ms", sub_start_s * 1000 + 3000) / 1000.0
                        
                        # ğŸ”§ ä¿®å¤4: æ›´æ–°æ€»ç»“æŸæ—¶é—´
                        total_end_s = max(total_end_s, sub_end_s)
                        
                        try:
                            # **äº‹ä»¶é©±åŠ¨æ£€æµ‹**
                            events = self._detect_events_in_segment(
                                sub_start_s, sub_end_s, video_path, precomputed_audio, precomputed_video
                            )
                            
                            trigger_info = self.detector.should_trigger_analysis(events, sub_start_s)
                            
                            # æ ¹æ®è§¦å‘ç»“æœå†³å®šå¤„ç†ç­–ç•¥
                            if trigger_info["should_trigger"]:
                                logger.info(f"ğŸ”¥ è§¦å‘é‡è¿ç®— {sub_start_s:.2f}s: {trigger_info['reasons']}")
                                # æ‰§è¡Œå®Œæ•´åˆ†æå¹¶ç¼“å­˜
                                segment_result = self._analyze_segment_full(
                                    sub_seg, audio_path, video_path, sub_start_s, sub_end_s
                                )
                                self._cache_segment_features(sub_start_s, segment_result)
                            else:
                                logger.info(f"ğŸ“‹ ä½¿ç”¨ç¼“å­˜æ’å€¼ {sub_start_s:.2f}s")
                                # ä½¿ç”¨ç¼“å­˜æ’å€¼
                                segment_result = self._analyze_segment_cached(
                                    sub_seg, sub_start_s, sub_end_s
                                )
                            
                            processed_segments.append(segment_result)
                            
                        except Exception as e:
                            logger.error(f"âŒ å¤„ç†å­æ®µè½ {sub_start_s:.2f}-{sub_end_s:.2f}s å¤±è´¥: {e}")
                            # ğŸ”§ ä¿®å¤5: æ·»åŠ å®¹é”™æœºåˆ¶ï¼Œåˆ›å»ºé»˜è®¤æ®µè½
                            default_segment = self._create_default_segment(sub_seg, sub_start_s, sub_end_s)
                            processed_segments.append(default_segment)
                            
                except Exception as e:
                    logger.error(f"âŒ å¤„ç†æ®µè½ {seg_idx+1} å¤±è´¥: {e}")
                    # åˆ›å»ºé»˜è®¤æ®µè½ç»§ç»­å¤„ç†
                    default_segment = self._create_default_segment(asr_seg, start_s, end_s)
                    processed_segments.append(default_segment)
        
        except Exception as e:
            logger.error(f"âŒ æ®µè½å¤„ç†å¾ªç¯å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
        
        # ğŸ”§ ä¿®å¤6: ä½¿ç”¨æ­£ç¡®çš„å˜é‡åè¿›è¡Œæ¸…ç†
        logger.info(f"ğŸ§¹ æ¸…ç†ç¼“å­˜ï¼Œæ€»æ—¶é•¿: {total_end_s:.2f}s")
        try:
            self.cache.cleanup_old_cache(total_end_s, self.CFG["cache_max_age_s"])
        except Exception as e:
            logger.warning(f"ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")
        
        return processed_segments
    
    def _detect_events_in_segment(self, start_s: float, end_s: float, video_path: str,
                                precomputed_audio: Dict, precomputed_video: Dict) -> List[Dict]:
        """æ£€æµ‹æ®µè½å†…çš„äº‹ä»¶"""
        events = []
        
        try:
            # æ£€æµ‹éŸ³é¢‘äº‹ä»¶
            audio_events = self.detector.detect_audio_events(precomputed_audio, start_s, end_s)
            events.extend(audio_events)
            
            # æ£€æµ‹è§†é¢‘äº‹ä»¶ï¼ˆè½¬æ¢ä¸ºå¸§ï¼‰
            fps = precomputed_video.get("fps", 30)
            start_frame = int(start_s * fps)
            end_frame = int(end_s * fps)
            video_events = self.detector.detect_video_events(video_path, start_frame, end_frame)
            events.extend(video_events)
            
        except Exception as e:
            logger.warning(f"äº‹ä»¶æ£€æµ‹å¤±è´¥: {e}")
        
        return events
    
    def _analyze_segment_full(self, segment: Dict, audio_path: str, video_path: str,
                            start_s: float, end_s: float) -> Dict:
        """æ‰§è¡Œå®Œæ•´çš„æ®µè½åˆ†æ"""
        start_ms = int(start_s * 1000)
        end_ms = int(end_s * 1000)
        
        try:
            # éŸµå¾‹åˆ†æ
            if hasattr(self.prosody_analyzer, 'analyze_segment'):
                sound_features = self.prosody_analyzer.analyze_segment(
                    audio_path, start_ms, end_ms, n_points=self.CFG["prosody_points"]
                )
            else:
                sound_features = MockProsodyAnalyzer.analyze_segment(
                    audio_path, start_ms, end_ms, self.CFG["prosody_points"]
                )
            
            # é¢éƒ¨åˆ†æ
            if hasattr(self.facial_analyzer, 'analyze_segment'):
                face_features = self.facial_analyzer.analyze_segment(video_path, start_ms, end_ms)
            else:
                face_features = MockFacialAnalyzer.analyze_segment(video_path, start_ms, end_ms)
            
        except Exception as e:
            logger.error(f"å®Œæ•´åˆ†æå¤±è´¥: {e}")
            sound_features = self._get_default_sound_features()
            face_features = self._get_default_face_features()
        
        # æ„å»ºç»“æœ
        result = {
            "segment_id": f"seg_{start_ms:06d}_{end_ms:06d}",
            "word": segment.get("text", ""),
            "start_ms": start_ms,
            "end_ms": end_ms,
            "sound": sound_features,
            "face": face_features,
            "meta": {
                "schema_version": "1.2.0",
                "source": segment.get("source", "ASR_PUNCT"),
                "fallback_vad": False,
                "original_punct": segment.get("punct", ""),
                "analysis_mode": "full_compute"
            }
        }
        
        return result
    
    def _analyze_segment_cached(self, segment: Dict, start_s: float, end_s: float) -> Dict:
        """ä½¿ç”¨ç¼“å­˜æ’å€¼åˆ†ææ®µè½"""
        start_ms = int(start_s * 1000)
        end_ms = int(end_s * 1000)
        
        # ä»ç¼“å­˜è·å–æ’å€¼ç‰¹å¾
        sound_features = self.cache.get_interpolated_audio(start_s, self.CFG["prosody_points"])
        face_features = self.cache.get_interpolated_video(start_s)
        
        # å¦‚æœç¼“å­˜ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤å€¼
        if sound_features is None:
            sound_features = self._get_default_sound_features()
        if face_features is None:
            face_features = self._get_default_face_features()
        
        result = {
            "segment_id": f"seg_{start_ms:06d}_{end_ms:06d}",
            "word": segment.get("text", ""),
            "start_ms": start_ms,
            "end_ms": end_ms,
            "sound": sound_features,
            "face": face_features,
            "meta": {
                "schema_version": "1.2.0",
                "source": segment.get("source", "ASR_PUNCT"),
                "fallback_vad": False,
                "original_punct": segment.get("punct", ""),
                "analysis_mode": "cached_interpolation"
            }
        }
        
        return result
    
    def _create_default_segment(self, segment: Dict, start_s: float, end_s: float) -> Dict:
        """ğŸ”§ æ–°å¢: åˆ›å»ºé»˜è®¤æ®µè½ï¼ˆå®¹é”™æœºåˆ¶ï¼‰"""
        start_ms = int(start_s * 1000)
        end_ms = int(end_s * 1000)
        
        return {
            "segment_id": f"seg_{start_ms:06d}_{end_ms:06d}",
            "word": segment.get("text", "å¤„ç†å¤±è´¥çš„æ®µè½"),
            "start_ms": start_ms,
            "end_ms": end_ms,
            "sound": self._get_default_sound_features(),
            "face": self._get_default_face_features(),
            "meta": {
                "schema_version": "1.2.0",
                "source": segment.get("source", "ERROR_FALLBACK"),
                "fallback_vad": False,
                "original_punct": segment.get("punct", ""),
                "analysis_mode": "error_fallback"
            }
        }
    
    def _cache_segment_features(self, timestamp: float, segment_result: Dict):
        """ç¼“å­˜æ®µè½ç‰¹å¾"""
        try:
            sound_features = segment_result.get("sound", {})
            face_features = segment_result.get("face", {})
            
            if sound_features:
                self.cache.store_audio_features(timestamp, {"sound": sound_features})
            if face_features:
                self.cache.store_video_features(timestamp, {"face": face_features})
        except Exception as e:
            logger.warning(f"ç‰¹å¾ç¼“å­˜å¤±è´¥: {e}")
    
    def _vad_split_segment_cached(self, start_s: float, end_s: float, segment: Dict) -> List[Dict]:
        """ä½¿ç”¨é¢„è®¡ç®—çš„VADä¿¡æ¯åˆ†å‰²é•¿æ®µè½"""
        if not self._precomputed_vad:
            return [segment]
        
        try:
            vad_timeline = self._precomputed_vad["timeline"]
            
            # åœ¨VADæ—¶é—´çº¿ä¸­æ‰¾åˆ°åˆ†å‰²ç‚¹
            split_points = []
            for t, vad_type in vad_timeline.items():
                if start_s < t < end_s and vad_type == "pause":
                    split_points.append(t)
            
            if not split_points:
                return [segment]
            
            # åˆ›å»ºå­æ®µè½
            sub_segments = []
            prev_start = start_s
            
            for split_point in split_points:
                if split_point - prev_start > 1.0:  # è‡³å°‘1ç§’
                    sub_seg = segment.copy()
                    sub_seg["start_ms"] = int(prev_start * 1000)
                    sub_seg["end_ms"] = int(split_point * 1000)
                    sub_seg["source"] = "VAD_SPLIT"
                    sub_segments.append(sub_seg)
                    prev_start = split_point
            
            # æœ€åä¸€æ®µ
            if end_s - prev_start > 1.0:
                sub_seg = segment.copy()
                sub_seg["start_ms"] = int(prev_start * 1000)
                sub_seg["end_ms"] = int(end_s * 1000)
                sub_seg["source"] = "VAD_SPLIT"
                sub_segments.append(sub_seg)
            
            return sub_segments
            
        except Exception as e:
            logger.warning(f"VADåˆ†å‰²å¤±è´¥: {e}")
            return [segment]
    
    def _extract_face_features_fast(self, frame: np.ndarray) -> Optional[Dict]:
        """å¿«é€Ÿæå–äººè„¸ç‰¹å¾ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        if self.face_cascade is None:
            return None
            
        try:
            # ç¼©å°å¸§ä»¥æé«˜é€Ÿåº¦
            small_frame = cv2.resize(frame, (320, 240))
            
            # ä½¿ç”¨å·²åˆå§‹åŒ–çš„çº§è”åˆ†ç±»å™¨
            faces = self.face_cascade.detectMultiScale(small_frame, 1.1, 4)
            
            if len(faces) == 0:
                return None
            
            # ç®€åŒ–ç‰¹å¾ï¼šåªæå–åŸºç¡€ä¿¡æ¯
            (x, y, w, h) = faces[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªäººè„¸
            
            # è®¡ç®—åŸºç¡€ç‰¹å¾
            face_center_x = (x + w/2) / small_frame.shape[1]
            face_center_y = (y + h/2) / small_frame.shape[0]
            face_size = (w * h) / (small_frame.shape[0] * small_frame.shape[1])
            
            # ç®€åŒ–çš„ç‰¹å¾å­—å…¸ï¼ˆmean, stdæ ¼å¼ï¼‰
            features = {
                "Smile": [0.5, 0.1],      # é»˜è®¤å€¼ï¼Œéœ€è¦å®é™…è®¡ç®—
                "Mouth": [0.3, 0.1], 
                "EAR": [0.25, 0.05],
                "Brow": [0.1, 0.02],
                "Yaw": [face_center_x * 20 - 10, 2.0],
                "Pitch": [face_center_y * 20 - 10, 2.0],
                "Roll": [0.0, 1.0],
                "FaceSize": [face_size, 0.1]
            }
            
            return features
            
        except Exception as e:
            logger.warning(f"å¿«é€Ÿäººè„¸ç‰¹å¾æå–å¤±è´¥: {e}")
            return None
    
    def _get_default_sound_features(self) -> Dict:
        """è·å–é»˜è®¤éŸ³é¢‘ç‰¹å¾"""
        n_points = self.CFG["prosody_points"]
        return {
            "pitch": [0.0] * n_points,
            "rate": [0.0] * n_points, 
            "level": [0.0] * n_points
        }
    
    def _get_default_face_features(self) -> Dict:
        """è·å–é»˜è®¤é¢éƒ¨ç‰¹å¾"""
        return {
            "Smile": [0.0, 0.0],
            "Mouth": [0.0, 0.0],
            "EAR": [0.0, 0.0],
            "Brow": [0.0, 0.0],
            "Yaw": [0.0, 0.0],
            "Pitch": [0.0, 0.0],
            "Roll": [0.0, 0.0]
        }
    
    def _build_vad_timeline(self, speech_segments: List, pause_segments: List) -> Dict:
        """æ„å»ºVADæ—¶é—´çº¿ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        timeline = {}
        
        try:
            # é™ä½æ—¶é—´çº¿ç²¾åº¦ä»¥èŠ‚çœå†…å­˜ï¼ˆä»0.1sæ”¹ä¸º0.5sï¼‰
            time_step = 0.5
            
            for speech_seg in speech_segments:
                start = speech_seg['start_time']
                end = speech_seg['end_time']
                for t in np.arange(start, end, time_step):
                    timeline[round(t, 1)] = "speech"
            
            for pause_seg in pause_segments:
                start = pause_seg['start_time']
                end = pause_seg['end_time']
                for t in np.arange(start, end, time_step):
                    timeline[round(t, 1)] = "pause"
                    
        except Exception as e:
            logger.warning(f"VADæ—¶é—´çº¿æ„å»ºå¤±è´¥: {e}")
                
        return timeline
    
    def _get_audio_duration(self, audio_path: str) -> float:
        """è·å–éŸ³é¢‘æ—¶é•¿ï¼ˆé¿å…é‡å¤åŠ è½½ï¼‰"""
        try:
            if HAS_LIBROSA:
                y, sr = librosa.load(audio_path, sr=None)
                return len(y) / sr
            else:
                # ä½¿ç”¨ffprobeè·å–æ—¶é•¿
                return self._get_duration_ffprobe(audio_path)
        except Exception as e:
            logger.warning(f"è·å–éŸ³é¢‘æ—¶é•¿å¤±è´¥: {e}")
            return 30.0  # é»˜è®¤30ç§’
    
    def _get_duration_ffprobe(self, audio_path: str) -> float:
        """ä½¿ç”¨ffprobeè·å–éŸ³é¢‘æ—¶é•¿"""
        try:
            cmd = [
                'ffprobe', '-i', audio_path, '-show_entries', 
                'format=duration', '-v', 'quiet', '-of', 'csv=p=0'
            ]
            result = subprocess.run(cmd, capture_output=True, text=True, check=True, timeout=10)
            return float(result.stdout.strip())
        except Exception as e:
            logger.warning(f"ffprobeè·å–æ—¶é•¿å¤±è´¥: {e}")
            return 30.0
    
    def _extract_audio(self, video_path: str) -> str:
        """ä»è§†é¢‘æå–éŸ³é¢‘"""
        logger.info("ğŸ¬ ä»è§†é¢‘æå–éŸ³é¢‘...")
        audio_path = video_path.rsplit('.', 1)[0] + "_extracted.wav"
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if os.path.exists(audio_path):
            logger.info(f"éŸ³é¢‘æ–‡ä»¶å·²å­˜åœ¨: {audio_path}")
            return audio_path
        
        try:
            # å…ˆéªŒè¯è§†é¢‘æ–‡ä»¶
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
            cap.release()
            
            # ä½¿ç”¨ffmpegæå–éŸ³é¢‘
            cmd = [
                "ffmpeg", "-i", video_path, "-acodec", "pcm_s16le", 
                "-ar", "16000", "-ac", "1", "-y", audio_path
            ]
            
            result = subprocess.run(cmd, check=True, capture_output=True, timeout=60)
            logger.info(f"âœ… éŸ³é¢‘æå–å®Œæˆ: {audio_path}")
            return audio_path
            
        except subprocess.TimeoutExpired:
            logger.error("éŸ³é¢‘æå–è¶…æ—¶")
            raise
        except subprocess.CalledProcessError as e:
            logger.error(f"éŸ³é¢‘æå–å¤±è´¥: {e}")
            logger.error(f"stderr: {e.stderr.decode() if e.stderr else 'No error message'}")
            raise
        except Exception as e:
            logger.error(f"éŸ³é¢‘æå–å¤±è´¥: {e}")
            raise
    
    def _fallback_vad_segmentation(self, audio_path: str) -> List[Dict]:
        """VADå…œåº•åˆ†æ®µ"""
        logger.info("ğŸ”„ ä½¿ç”¨VADå…œåº•åˆ†æ®µ...")
        
        try:
            if not self.pause_detector:
                # åˆ›å»ºé»˜è®¤æ®µè½
                duration = self._get_audio_duration(audio_path)
                return [{
                    "text": "æ— æ³•è¯†åˆ«éŸ³é¢‘å†…å®¹",
                    "start_ms": 0,
                    "end_ms": int(duration * 1000),
                    "source": "FALLBACK"
                }]
            
            result = self.pause_detector.detect_pauses_from_file(audio_path)
            speech_segments = result['speech_segments']
            
            segments = []
            for i, speech_seg in enumerate(speech_segments):
                segments.append({
                    "text": f"è¯­éŸ³æ®µè½{i+1}",
                    "start_ms": int(speech_seg['start_time'] * 1000),
                    "end_ms": int(speech_seg['end_time'] * 1000), 
                    "source": "VAD_FALLBACK"
                })
            
            return segments
            
        except Exception as e:
            logger.error(f"VADå…œåº•åˆ†æ®µå¤±è´¥: {e}")
            # æœ€ç»ˆå…œåº•ï¼šåˆ›å»ºå•ä¸ªé»˜è®¤æ®µè½
            duration = self._get_audio_duration(audio_path)
            return [{
                "text": "åˆ†æ®µå¤±è´¥ï¼Œä½¿ç”¨æ•´ä¸ªéŸ³é¢‘",
                "start_ms": 0,
                "end_ms": int(duration * 1000),
                "source": "ULTIMATE_FALLBACK"
            }]


# =============================================================================
# ä¾¿åˆ©å‡½æ•°å’Œæµ‹è¯•æ¥å£
# =============================================================================

def create_analyzer(api_key: str = None, **kwargs) -> MultiModalAnalyzer_Optimized:
    """ä¾¿åˆ©å‡½æ•°ï¼šåˆ›å»ºåˆ†æå™¨å®ä¾‹"""
    return MultiModalAnalyzer_Optimized(api_key=api_key, **kwargs)

def analyze_video(video_path: str, api_key: str = None, **kwargs) -> Dict:
    """ä¾¿åˆ©å‡½æ•°ï¼šç›´æ¥åˆ†æè§†é¢‘"""
    analyzer = create_analyzer(api_key, **kwargs)
    return analyzer.analyze_video_audio(video_path, extract_audio=True)

if __name__ == "__main__":
    # ç®€å•æµ‹è¯•
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python MultiModalAnalyzer_Optimized.py <è§†é¢‘æ–‡ä»¶è·¯å¾„>")
        sys.exit(1)
    
    video_path = sys.argv[1]
    
    try:
        print("ğŸš€ å¼€å§‹åˆ†æ...")
        result = analyze_video(video_path)
        
        print(f"âœ… åˆ†æå®Œæˆ!")
        print(f"   æ€»æ—¶é•¿: {result['summary']['total_duration']:.2f}s")
        print(f"   æ®µè½æ•°: {result['summary']['segment_count']}")
        print(f"   å¤„ç†æ—¶é—´: {result['summary']['performance']['total_time_s']:.2f}s")
        print(f"   åŠ é€Ÿæ¯”: {result['summary']['performance']['speedup_ratio']}")
        
        # ä¿å­˜ç»“æœ
        output_path = video_path.rsplit('.', 1)[0] + "_analysis_result.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        traceback.print_exc()
        sys.exit(1)