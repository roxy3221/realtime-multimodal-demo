/**
 * MediaCapture - åª’ä½“é‡‡é›†å’Œå¤„ç†ç®¡é“
 * åŸºäºäº‹ä»¶é©±åŠ¨æ¶æ„ï¼Œå®ç°é«˜æ•ˆçš„éŸ³è§†é¢‘æµå¤„ç†
 */

import type { FaceEvent, ProsodyEvent, MediaConfig } from '../types';
import { EventBus } from '../events/EventBus';
import { DEFAULT_MEDIA_CONFIG } from '../config/defaults';
import { WebSpeechASR } from '../asr/WebSpeechASR';

export class MediaCapture {
  private stream: MediaStream | null = null;
  private videoElement: HTMLVideoElement | null = null;
  private audioContext: AudioContext | null = null;
  private audioWorklet: AudioWorkletNode | null = null;
  private videoWorker: Worker | null = null;
  private asr: WebSpeechASR | null = null;
  private eventBus: EventBus;
  private isCapturing = false;
  private animationFrame: number | null = null;
  private offscreenCanvas: OffscreenCanvas | null = null;

  constructor(eventBus: EventBus) {
    this.eventBus = eventBus;
  }

  /**
   * åˆå§‹åŒ–åª’ä½“é‡‡é›†
   */
  async initialize(config: Partial<MediaConfig> = {}): Promise<void> {
    const finalConfig = {
      ...DEFAULT_MEDIA_CONFIG,
      ...config
    };

    try {
      // 1. è·å–åª’ä½“æµ
      await this.setupMediaStream(finalConfig);
      
      // 2. åˆå§‹åŒ–éŸ³é¢‘å¤„ç†ç®¡é“
      await this.setupAudioPipeline(finalConfig);
      
      // 3. åˆå§‹åŒ–è§†é¢‘å¤„ç†ç®¡é“
      await this.setupVideoPipeline(finalConfig);
      
      // 4. åˆå§‹åŒ–ASR
      this.setupASR();
      
      console.log('âœ… MediaCapture initialized successfully');
    } catch (error) {
      console.error('âŒ MediaCapture initialization failed:', error);
      throw error;
    }
  }

  /**
   * è®¾ç½®åª’ä½“æµ
   */
  private async setupMediaStream(config: MediaConfig): Promise<void> {
    const constraints: MediaStreamConstraints = {
      video: {
        width: { ideal: config.video.width },
        height: { ideal: config.video.height },
        frameRate: { ideal: config.video.frameRate },
        facingMode: config.video.facingMode
      },
      audio: {
        sampleRate: { ideal: config.audio.sampleRate },
        channelCount: { exact: config.audio.channelCount },
        echoCancellation: config.audio.echoCancellation,
        noiseSuppression: config.audio.noiseSuppression
      }
    };

    this.stream = await navigator.mediaDevices.getUserMedia(constraints);
    
    // åˆ›å»ºvideoå…ƒç´ ç”¨äºé¢„è§ˆå’Œå¸§æå–
    this.videoElement = document.createElement('video');
    this.videoElement.srcObject = this.stream;
    this.videoElement.autoplay = true;
    this.videoElement.muted = true;
    this.videoElement.playsInline = true;
    
    // ç­‰å¾…è§†é¢‘å‡†å¤‡å°±ç»ª
    await new Promise<void>((resolve) => {
      this.videoElement!.onloadedmetadata = () => resolve();
    });
  }

  /**
   * è®¾ç½®éŸ³é¢‘å¤„ç†ç®¡é“
   */
  private async setupAudioPipeline(config: MediaConfig): Promise<void> {
    if (!this.stream) throw new Error('Stream not initialized');

    const audioTrack = this.stream.getAudioTracks()[0];
    if (!audioTrack) {
      console.warn('âš ï¸ No audio track available');
      return;
    }

    // åˆ›å»ºAudioContext
    this.audioContext = new AudioContext({
      sampleRate: config.audio.sampleRate
    });

    // åŠ è½½AudioWorkletå¤„ç†å™¨
    await this.audioContext.audioWorklet.addModule('/workers/audio-processor.js');
    
    // åˆ›å»ºåª’ä½“æºå’Œå¤„ç†èŠ‚ç‚¹
    const source = this.audioContext.createMediaStreamSource(this.stream);
    
    this.audioWorklet = new AudioWorkletNode(this.audioContext, 'audio-processor', {
      numberOfInputs: 1,
      numberOfOutputs: 0,
      channelCount: config.audio.channelCount,
      processorOptions: {
        sampleRate: config.audio.sampleRate,
        windowSize: config.audio.windowSize || 1024
      }
    });

    // ç›‘å¬éŸµå¾‹äº‹ä»¶
    this.audioWorklet.port.onmessage = (event) => {
      const { type, data } = event.data;
      
      switch (type) {
        case 'prosody-event':
          this.eventBus.publish({
            type: 'prosody',
            t: Date.now(),
            deltaScore: data.deltaScore,
            rms: data.rms,
            f0: data.f0,
            wpm: data.wpm || 0
          } as ProsodyEvent);
          break;
        
        case 'vad-change':
          // VADçŠ¶æ€å˜åŒ–ï¼Œå¯ç”¨äºASRæ§åˆ¶
          // æš‚æ—¶åªè®°å½•æ—¥å¿—ï¼Œä¸å‘é€äº‹ä»¶
          console.log('VADçŠ¶æ€å˜åŒ–:', data);
          break;
      }
    };

    // è¿æ¥éŸ³é¢‘ç®¡é“
    source.connect(this.audioWorklet);
  }

  /**
   * åˆ›å»ºä¼ ç»ŸWorkerï¼Œä½¿ç”¨Blob URLæ–¹å¼è§£å†³importScriptsé—®é¢˜
   */
  private createTraditionalWorker(): Worker {
    const workerCode = `
      // ä¼ ç»ŸWorkerä»£ç ï¼Œæ”¯æŒimportScripts
      importScripts('https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest');
      
      let faceLandmarker = null;
      let isInitialized = false;
      let canvas = null;
      let ctx = null;
      
      // åˆå§‹åŒ–MediaPipe Face Landmarker
      const initializeFaceLandmarker = async () => {
        try {
          const { FilesetResolver, FaceLandmarker } = self;
          
          const vision = await FilesetResolver.forVisionTasks(
            "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm"
          );
          
          faceLandmarker = await FaceLandmarker.createFromOptions(vision, {
            baseOptions: {
              modelAssetPath: "https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task",
              delegate: "GPU"
            },
            runningMode: "VIDEO",
            numFaces: 1,
            outputFaceBlendshapes: true,
            outputFacialTransformationMatrixes: true
          });
          
          isInitialized = true;
          self.postMessage({ type: 'ready' });
        } catch (error) {
          console.error('FaceLandmarker initialization failed:', error);
          self.postMessage({ 
            type: 'error', 
            error: error.message || 'Failed to initialize face detection' 
          });
        }
      };
      
      // å¤„ç†è§†é¢‘å¸§
      const processFrame = (imageData, timestamp, videoWidth, videoHeight) => {
        if (!isInitialized || !faceLandmarker) {
          return;
        }
        
        try {
          // å¦‚æœcanvaså°ºå¯¸å‘ç”Ÿå˜åŒ–ï¼Œéœ€è¦é‡æ–°åˆ›å»º
          if (!canvas || canvas.width !== videoWidth || canvas.height !== videoHeight) {
            canvas = new OffscreenCanvas(videoWidth, videoHeight);
            ctx = canvas.getContext('2d');
          }
          
          // å°†ImageDataç»˜åˆ¶åˆ°canvas
          ctx.putImageData(imageData, 0, 0);
          
          // MediaPipeå¤„ç†
          const results = faceLandmarker.detectForVideo(canvas, timestamp);
          
          if (results.faceLandmarks && results.faceLandmarks.length > 0) {
            // æå–ç¬¬ä¸€ä¸ªäººè„¸çš„æ•°æ®
            const landmarks = results.faceLandmarks[0];
            const blendshapes = results.faceBlendshapes && results.faceBlendshapes.length > 0
              ? results.faceBlendshapes[0]
              : null;
            
            // è®¡ç®—åŸºæœ¬è¡¨æƒ…åˆ†æ•°
            const expr = {};
            if (blendshapes && blendshapes.categories) {
              blendshapes.categories.forEach(category => {
                expr[category.categoryName] = category.score;
              });
            }
            
            // ç®€å•çš„å§¿æ€ä¼°è®¡ (åŸºäºå…³é”®ç‚¹)
            const pose = calculateSimplePose(landmarks);
            
            // è®¡ç®—å˜åŒ–åˆ†æ•° (ç®€åŒ–ç‰ˆæœ¬)
            const deltaScore = calculateDeltaScore(expr, pose, timestamp);
            
            // å¦‚æœå˜åŒ–æ˜¾è‘—ï¼Œå‘é€äº‹ä»¶
            if (deltaScore > 0.3) {
              self.postMessage({
                type: 'face-event',
                data: {
                  deltaScore,
                  expr,
                  pose,
                  timestamp,
                  landmarks: landmarks.slice(0, 10) // åªå‘é€å‰10ä¸ªå…³é”®ç‚¹å‡å°‘æ•°æ®é‡
                }
              });
            }
          }
        } catch (error) {
          console.error('Frame processing error:', error);
        }
      };
      
      // ç®€å•çš„å§¿æ€è®¡ç®—
      const calculateSimplePose = (landmarks) => {
        // ä½¿ç”¨å…³é”®ç‚¹è®¡ç®—ç®€å•çš„å¤´éƒ¨å§¿æ€
        // è¿™é‡Œæ˜¯ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…é¡¹ç›®ä¸­åº”è¯¥ä½¿ç”¨æ›´å‡†ç¡®çš„ç®—æ³•
        const nose = landmarks[1];
        const leftEye = landmarks[33];
        const rightEye = landmarks[362];
        
        // è®¡ç®—Rollè§’åº¦
        const eyeDiff = {
          x: rightEye.x - leftEye.x,
          y: rightEye.y - leftEye.y
        };
        const roll = Math.atan2(eyeDiff.y, eyeDiff.x) * (180 / Math.PI);
        
        return {
          yaw: 0,   // ç®€åŒ–ä¸º0ï¼Œå®é™…éœ€è¦æ›´å¤æ‚è®¡ç®—
          pitch: 0, // ç®€åŒ–ä¸º0ï¼Œå®é™…éœ€è¦æ›´å¤æ‚è®¡ç®—
          roll: roll
        };
      };
      
      // å˜åŒ–åˆ†æ•°è®¡ç®— (ç®€åŒ–ç‰ˆæœ¬)
      let lastExpr = {};
      let lastPose = { yaw: 0, pitch: 0, roll: 0 };
      let lastTimestamp = 0;
      
      const calculateDeltaScore = (expr, pose, timestamp) => {
        if (timestamp - lastTimestamp < 100) { // é™åˆ¶é¢‘ç‡
          return 0;
        }
        
        let exprDelta = 0;
        let poseDelta = 0;
        
        // è¡¨æƒ…å˜åŒ–
        Object.keys(expr).forEach(key => {
          const curr = expr[key] || 0;
          const prev = lastExpr[key] || 0;
          exprDelta += Math.abs(curr - prev);
        });
        
        // å§¿æ€å˜åŒ–
        poseDelta = Math.abs(pose.roll - lastPose.roll) / 90; // å½’ä¸€åŒ–åˆ°0-1
        
        // æ›´æ–°å†å²
        lastExpr = { ...expr };
        lastPose = { ...pose };
        lastTimestamp = timestamp;
        
        return Math.min(exprDelta + poseDelta, 1.0);
      };
      
      // æ¶ˆæ¯å¤„ç†
      self.onmessage = async (event) => {
        const { type, data } = event.data;
        
        switch (type) {
          case 'init':
            await initializeFaceLandmarker();
            break;
            
          case 'process-frame':
            if (data.imageData) {
              processFrame(
                data.imageData, 
                data.timestamp, 
                data.videoWidth, 
                data.videoHeight
              );
            }
            break;
            
          case 'destroy':
            // æ¸…ç†èµ„æº
            faceLandmarker = null;
            isInitialized = false;
            canvas = null;
            ctx = null;
            break;
            
          default:
            console.warn('Unknown message type:', type);
        }
      };
    `;
    
    // åˆ›å»ºBlob URL
    const blob = new Blob([workerCode], { type: 'application/javascript' });
    const workerUrl = URL.createObjectURL(blob);
    
    // åˆ›å»ºä¼ ç»ŸWorker
    const worker = new Worker(workerUrl);
    
    // æ¸…ç†URL (å¯é€‰ï¼Œé¿å…å†…å­˜æ³„æ¼)
    worker.addEventListener('error', () => {
      URL.revokeObjectURL(workerUrl);
    });
    
    return worker;
  }

  /**
   * è®¾ç½®è§†é¢‘å¤„ç†ç®¡é“
   */
  private async setupVideoPipeline(_config: MediaConfig): Promise<void> {
    if (!this.videoElement) throw new Error('Video element not initialized');

    // åˆ›å»ºä¼ ç»ŸWorkerè€Œä¸æ˜¯æ¨¡å—Worker
    this.videoWorker = this.createTraditionalWorker();

    // åˆå§‹åŒ–Worker
    this.videoWorker.postMessage({
      type: 'init'
    });

    // ç›‘å¬Workeræ¶ˆæ¯
    this.videoWorker.onmessage = (event) => {
      const { type, data } = event.data;
      
      switch (type) {
        case 'ready':
          console.log('âœ… Face detection worker ready');
          break;
          
        case 'face-event':
          this.eventBus.publish({
            type: 'face',
            t: Date.now(),
            timestamp: Date.now(),
            deltaScore: data.deltaScore,
            expression: data.expr,
            pose: data.pose,
            confidence: data.confidence || 0.8
          } as FaceEvent);
          break;
          
        case 'error':
          console.error('âŒ Face detection worker error:', data.error);
          break;
      }
    };

    this.videoWorker.onerror = (error) => {
      console.error('âŒ Face detection worker error:', error);
    };
  }

  /**
   * è®¾ç½®ASR
   */
  private setupASR(): void {
    this.asr = new WebSpeechASR(this.eventBus);
  }

  /**
   * å¼€å§‹é‡‡é›†
   */
  async startCapture(): Promise<void> {
    if (this.isCapturing) return;
    
    this.isCapturing = true;
    
    // å¯åŠ¨éŸ³é¢‘ä¸Šä¸‹æ–‡
    if (this.audioContext?.state === 'suspended') {
      await this.audioContext.resume();
    }
    
    // å¯åŠ¨ASR
    if (this.asr) {
      this.asr.start();
    }
    
    // å¯åŠ¨è§†é¢‘å¸§å¤„ç†å¾ªç¯
    this.startVideoFrameLoop();
    
    console.log('ğŸ¥ Media capture started');
  }

  /**
   * è§†é¢‘å¸§å¤„ç†å¾ªç¯
   */
  private startVideoFrameLoop(): void {
    if (!this.isCapturing || !this.videoElement || !this.offscreenCanvas) return;

    // åˆ›å»ºä¸»çº¿ç¨‹canvasç”¨äºå¸§æ•è·
    const tempCanvas = document.createElement('canvas');
    const tempCtx = tempCanvas.getContext('2d');
    
    if (!tempCtx) {
      console.error('âŒ Cannot get 2d context for video frame capture');
      return;
    }

    const processFrame = () => {
      if (!this.isCapturing || !this.videoElement || !this.videoWorker || !tempCtx) return;

      // æ£€æŸ¥è§†é¢‘æ˜¯å¦æœ‰æ–°å¸§
      if (this.videoElement.readyState >= 2 && this.videoElement.videoWidth > 0) {
        const { videoWidth, videoHeight } = this.videoElement;
        
        // è°ƒæ•´ä¸´æ—¶canvaså¤§å°
        if (tempCanvas.width !== videoWidth || tempCanvas.height !== videoHeight) {
          tempCanvas.width = videoWidth;
          tempCanvas.height = videoHeight;
        }
        
        // å°†è§†é¢‘å¸§ç»˜åˆ¶åˆ°ä¸´æ—¶canvas
        tempCtx.drawImage(this.videoElement, 0, 0, videoWidth, videoHeight);
        
        // è·å–ImageData
        const imageData = tempCtx.getImageData(0, 0, videoWidth, videoHeight);
        
        // å‘é€ImageDataåˆ°Workerå¤„ç†
        this.videoWorker.postMessage({
          type: 'process-frame',
          timestamp: Date.now(),
          imageData: imageData,
          videoWidth,
          videoHeight
        });
      }

      if (this.isCapturing) {
        this.animationFrame = requestAnimationFrame(processFrame);
      }
    };

    processFrame();
  }

  /**
   * åœæ­¢é‡‡é›†
   */
  stopCapture(): void {
    this.isCapturing = false;
    
    // åœæ­¢åŠ¨ç”»å¸§å¾ªç¯
    if (this.animationFrame) {
      cancelAnimationFrame(this.animationFrame);
      this.animationFrame = null;
    }
    
    // åœæ­¢ASR
    if (this.asr) {
      this.asr.stop();
    }
    
    // æš‚åœéŸ³é¢‘ä¸Šä¸‹æ–‡
    this.audioContext?.suspend();
    
    console.log('â¹ï¸ Media capture stopped');
  }

  /**
   * è·å–é¢„è§ˆå…ƒç´ 
   */
  getPreviewElement(): HTMLVideoElement | null {
    return this.videoElement;
  }

  /**
   * è·å–åª’ä½“æµçŠ¶æ€
   */
  getStatus(): {
    isCapturing: boolean;
    hasVideo: boolean;
    hasAudio: boolean;
    audioContextState?: AudioContextState;
  } {
    return {
      isCapturing: this.isCapturing,
      hasVideo: !!this.stream?.getVideoTracks().length,
      hasAudio: !!this.stream?.getAudioTracks().length,
      audioContextState: this.audioContext?.state
    };
  }

  /**
   * æ¸…ç†èµ„æº
   */
  dispose(): void {
    this.stopCapture();
    
    // åœæ­¢æ‰€æœ‰åª’ä½“è½¨é“
    this.stream?.getTracks().forEach(track => track.stop());
    this.stream = null;
    
    // æ¸…ç†éŸ³é¢‘èµ„æº
    this.audioWorklet?.disconnect();
    this.audioContext?.close();
    this.audioContext = null;
    this.audioWorklet = null;
    
    // æ¸…ç†è§†é¢‘èµ„æº
    if (this.videoWorker) {
      this.videoWorker.postMessage({ type: 'destroy' });
      this.videoWorker.terminate();
      this.videoWorker = null;
    }
    
    if (this.videoElement) {
      this.videoElement.srcObject = null;
      this.videoElement = null;
    }
    
    this.offscreenCanvas = null;
    
    console.log('ğŸ§¹ MediaCapture disposed');
  }
}