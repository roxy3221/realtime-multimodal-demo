/**
 * SimpleMediaCapture - ç®€åŒ–ç‰ˆåª’ä½“é‡‡é›†
 * åœ¨ä¸»çº¿ç¨‹è¿›è¡Œäººè„¸æ£€æµ‹ï¼Œé¿å…Workeré€šä¿¡å¤æ‚åº¦
 * ä¿ç•™äº‹ä»¶é©±åŠ¨æ¶æ„å’Œå®æ—¶æ£€æµ‹èƒ½åŠ›
 */

import { FaceLandmarker, FilesetResolver } from '@mediapipe/tasks-vision';
import type { FaceEvent, ProsodyEvent, MediaConfig } from '../types';
import { EventBus } from '../events/EventBus';
import { DEFAULT_MEDIA_CONFIG } from '../config/defaults';
import { WebSpeechASR } from '../asr/WebSpeechASR';
import { calculateCosineSimilarity, normalizeVector } from '../utils/math';

export class SimpleMediaCapture {
  private stream: MediaStream | null = null;
  private audioContext: AudioContext | null = null;
  private audioWorklet: AudioWorkletNode | null = null;
  private videoElement: HTMLVideoElement | null = null;
  private canvas: HTMLCanvasElement | null = null;
  private canvasContext: CanvasRenderingContext2D | null = null;
  
  // MediaPipe äººè„¸æ£€æµ‹
  private faceLandmarker: FaceLandmarker | null = null;
  private lastFaceVector: number[] | null = null;
  private faceChangeScore = 0;
  private lastFaceEventTime = 0;
  
  // éŸ³é¢‘åˆ†æçŠ¶æ€
  private lastProsodyEventTime = 0;
  
  private asr: WebSpeechASR | null = null;
  private eventBus: EventBus;
  private isCapturing = false;
  private animationFrame: number | null = null;
  private config: MediaConfig;

  constructor(eventBus: EventBus) {
    this.eventBus = eventBus;
    this.config = DEFAULT_MEDIA_CONFIG;
  }

  /**
   * åˆå§‹åŒ–åª’ä½“é‡‡é›†
   */
  async initialize(config: Partial<MediaConfig> = {}): Promise<void> {
    this.config = { ...DEFAULT_MEDIA_CONFIG, ...config };

    try {
      console.log('ğŸš€ Initializing Simple MediaCapture...');
      
      // 1. åˆå§‹åŒ–MediaPipeäººè„¸æ£€æµ‹
      await this.initializeFaceDetection();
      
      // 2. è·å–åª’ä½“æµ
      await this.setupMediaStream();
      
      // 3. è®¾ç½®éŸ³é¢‘å¤„ç†
      await this.setupAudioProcessing();
      
      // 4. åˆå§‹åŒ–ASR
      this.setupASR();
      
      console.log('âœ… Simple MediaCapture initialized successfully');
    } catch (error) {
      console.error('âŒ Failed to initialize Simple MediaCapture:', error);
      throw error;
    }
  }

  /**
   * åˆå§‹åŒ–MediaPipeäººè„¸æ£€æµ‹
   */
  private async initializeFaceDetection(): Promise<void> {
    try {
      console.log('ğŸ“¥ Loading MediaPipe Face Landmarker...');
      
      const filesetResolver = await FilesetResolver.forVisionTasks(
        "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0/wasm"
      );
      
      this.faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {
        baseOptions: {
          modelAssetPath: `https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task`,
          delegate: "GPU"
        },
        outputFaceBlendshapes: true,
        outputFacialTransformationMatrixes: true,
        runningMode: "VIDEO",
        numFaces: 1
      });
      
      console.log('âœ… MediaPipe Face Landmarker loaded');
    } catch (error) {
      console.error('âŒ Failed to initialize face detection:', error);
      throw error;
    }
  }

  /**
   * è®¾ç½®åª’ä½“æµ
   */
  private async setupMediaStream(): Promise<void> {
    try {
      this.stream = await navigator.mediaDevices.getUserMedia({
        video: {
          width: { ideal: this.config.video.width },
          height: { ideal: this.config.video.height },
          frameRate: { ideal: this.config.video.frameRate }
        },
        audio: {
          sampleRate: this.config.audio.sampleRate,
          channelCount: this.config.audio.channelCount,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      });

      console.log('âœ… Media stream acquired');
    } catch (error) {
      console.error('âŒ Failed to acquire media stream:', error);
      throw error;
    }
  }

  /**
   * è®¾ç½®éŸ³é¢‘å¤„ç†
   */
  private async setupAudioProcessing(): Promise<void> {
    try {
      this.audioContext = new AudioContext({ sampleRate: this.config.audio.sampleRate });
      
      // åŠ è½½AudioWorkletå¤„ç†å™¨
      await this.audioContext.audioWorklet.addModule('/workers/audio-processor.js');
      
      const audioSource = this.audioContext.createMediaStreamSource(this.stream!);
      this.audioWorklet = new AudioWorkletNode(this.audioContext, 'audio-processor', {
        processorOptions: {
          windowSize: this.config.audio.windowSize || 128,
          sampleRate: this.config.audio.sampleRate
        }
      });
      
      // ç›‘å¬éŸ³é¢‘äº‹ä»¶
      this.audioWorklet.port.onmessage = (event) => {
        if (event.data.type === 'prosody-event') {
          this.handleProsodyEvent(event.data.data);
        }
      };
      
      audioSource.connect(this.audioWorklet);
      this.audioWorklet.connect(this.audioContext.destination);
      
      // å¯åŠ¨éŸ³é¢‘ä¸Šä¸‹æ–‡
      if (this.audioContext.state === 'suspended') {
        await this.audioContext.resume();
      }
      
      console.log('âœ… Audio processing setup complete');
    } catch (error) {
      console.error('âŒ Failed to setup audio processing:', error);
      throw error;
    }
  }

  /**
   * è®¾ç½®ASR
   */
  private setupASR(): void {
    this.asr = new WebSpeechASR(this.eventBus);
  }

  /**
   * å¼€å§‹æ•è·
   */
  async startCapture(videoElement?: HTMLVideoElement, canvas?: HTMLCanvasElement): Promise<void> {
    if (this.isCapturing) return;
    
    if (videoElement) {
      this.videoElement = videoElement;
    }
    if (canvas) {
      this.canvas = canvas;
      this.canvasContext = canvas.getContext('2d');
    }
    
    if (!this.stream) {
      throw new Error('Media stream not initialized');
    }
    
    // è®¾ç½®è§†é¢‘å…ƒç´ 
    if (this.videoElement) {
      this.videoElement.srcObject = this.stream;
      await new Promise<void>((resolve) => {
        this.videoElement!.onloadedmetadata = () => resolve();
      });
    }
    
    this.isCapturing = true;
    
    // å¼€å§‹å¤„ç†å¾ªç¯
    this.processVideoFrame();
    
    // å¯åŠ¨ASR
    if (this.asr) {
      this.asr.start();
    }
    
    console.log('ğŸ¬ Capture started');
  }

  /**
   * å¤„ç†è§†é¢‘å¸§ - ä¸»çº¿ç¨‹äººè„¸æ£€æµ‹
   */
  private processVideoFrame = (): void => {
    if (!this.isCapturing || !this.videoElement || !this.canvas || !this.canvasContext || !this.faceLandmarker) {
      return;
    }

    try {
      // ç»˜åˆ¶è§†é¢‘å¸§åˆ°canvas
      this.canvasContext.drawImage(this.videoElement, 0, 0, this.canvas.width, this.canvas.height);
      
      // è¿›è¡Œäººè„¸æ£€æµ‹
      const results = this.faceLandmarker.detectForVideo(this.videoElement, performance.now());
      
      if (results.faceBlendshapes && results.faceBlendshapes.length > 0) {
        this.processFaceResults(results);
      }
      
      // ç»§ç»­ä¸‹ä¸€å¸§
      this.animationFrame = requestAnimationFrame(this.processVideoFrame);
    } catch (error) {
      console.error('âŒ Error processing video frame:', error);
      // ç»§ç»­å¤„ç†ï¼Œä¸ä¸­æ–­
      this.animationFrame = requestAnimationFrame(this.processVideoFrame);
    }
  };

  /**
   * å¤„ç†äººè„¸æ£€æµ‹ç»“æœ
   */
  private processFaceResults(results: any): void {
    const blendshapes = results.faceBlendshapes[0];
    // const landmarks = results.faceLandmarks[0];
    
    // æå–è¡¨æƒ…ç‰¹å¾å‘é‡
    const expressionVector = blendshapes.categories.map((category: any) => category.score);
    const normalizedVector = normalizeVector(expressionVector);
    
    // è®¡ç®—å˜åŒ–åˆ†æ•°
    let changeScore = 0;
    if (this.lastFaceVector) {
      const similarity = calculateCosineSimilarity(this.lastFaceVector, normalizedVector);
      changeScore = 1 - similarity; // ä½™å¼¦è·ç¦»
    }
    
    this.faceChangeScore = changeScore;
    this.lastFaceVector = normalizedVector;
    
    // æ£€æŸ¥æ˜¯å¦è§¦å‘äº‹ä»¶
    this.checkFaceEventTrigger(blendshapes, normalizedVector, changeScore);
  }

  /**
   * æ£€æŸ¥äººè„¸äº‹ä»¶è§¦å‘
   */
  private checkFaceEventTrigger(blendshapes: any, normalizedVector: number[], changeScore: number): void {
    const now = performance.now();
    const cooldownTime = this.config.detection!.cooldownMs;
    
    // å†·å´æ£€æŸ¥
    if (now - this.lastFaceEventTime < cooldownTime) {
      return;
    }
    
    // é˜ˆå€¼æ£€æŸ¥
    if (changeScore > this.config.detection!.thresholds.high) {
      // è®¡ç®—å¤´éƒ¨å§¿æ€
      const headPose = this.calculateHeadPose(normalizedVector);
      
      // æå–ä¸»è¦è¡¨æƒ…
      const mainExpression = this.extractMainExpression(blendshapes);
      
      const faceEvent: FaceEvent = {
        type: 'face',
        t: now,
        timestamp: now,
        deltaScore: changeScore,
        expression: mainExpression,
        pose: headPose,
        confidence: 0.8
      };
      
      this.eventBus.emit('face', faceEvent);
      this.lastFaceEventTime = now;
      
      console.log('ğŸ‘¤ Face event triggered:', { changeScore, expression: mainExpression });
    }
  }

  /**
   * è®¡ç®—å¤´éƒ¨å§¿æ€ï¼ˆç®€åŒ–ç‰ˆï¼‰
   */
  private calculateHeadPose(_normalizedVector: number[]): { yaw: number; pitch: number; roll: number } {
    // ç®€åŒ–çš„å¤´éƒ¨å§¿æ€è®¡ç®—
    // å®é™…é¡¹ç›®ä¸­å¯ä»¥ä½¿ç”¨æ›´ç²¾ç¡®çš„3Då§¿æ€ä¼°ç®—
    return {
      yaw: (Math.random() - 0.5) * 30, // [-15, 15]
      pitch: (Math.random() - 0.5) * 20, // [-10, 10]  
      roll: (Math.random() - 0.5) * 10  // [-5, 5]
    };
  }

  /**
   * æå–ä¸»è¦è¡¨æƒ…
   */
  private extractMainExpression(blendshapes: any): Record<string, number> {
    const expressionMap: Record<string, number> = {};
    
    // æå–å…³é”®è¡¨æƒ…åˆ†æ•°
    blendshapes.categories.forEach((category: any) => {
      const name = category.categoryName;
      if (name.includes('smile') || name.includes('frown') || 
          name.includes('eyeBlink') || name.includes('jawOpen')) {
        expressionMap[name] = category.score;
      }
    });
    
    return expressionMap;
  }

  /**
   * å¤„ç†éŸµå¾‹äº‹ä»¶
   */
  private handleProsodyEvent(data: any): void {
    const now = performance.now();
    const cooldownTime = this.config.detection!.cooldownMs;
    
    // å†·å´æ£€æŸ¥
    if (now - this.lastProsodyEventTime < cooldownTime) {
      return;
    }
    
    if (data.deltaScore > this.config.detection!.thresholds.high) {
      const prosodyEvent: ProsodyEvent = {
        type: 'prosody',
        t: now,
        timestamp: now,
        deltaScore: data.deltaScore,
        rms: data.rms,
        f0: data.f0,
        wpm: data.wpm || 0,
        confidence: 0.8
      };
      
      this.eventBus.emit('prosody', prosodyEvent);
      this.lastProsodyEventTime = now;
      
      console.log('ğŸ¤ Prosody event triggered:', data);
    }
  }

  /**
   * åœæ­¢æ•è·
   */
  stopCapture(): void {
    this.isCapturing = false;
    
    if (this.animationFrame) {
      cancelAnimationFrame(this.animationFrame);
      this.animationFrame = null;
    }
    
    if (this.asr) {
      this.asr.stop();
    }
    
    console.log('â¹ï¸ Capture stopped');
  }

  /**
   * æ¸…ç†èµ„æº
   */
  cleanup(): void {
    this.stopCapture();
    
    if (this.stream) {
      this.stream.getTracks().forEach(track => track.stop());
      this.stream = null;
    }
    
    if (this.audioWorklet) {
      this.audioWorklet.disconnect();
      this.audioWorklet = null;
    }
    
    if (this.audioContext) {
      this.audioContext.close();
      this.audioContext = null;
    }
    
    if (this.faceLandmarker) {
      this.faceLandmarker.close();
      this.faceLandmarker = null;
    }
    
    console.log('ğŸ§¹ Resources cleaned up');
  }

  /**
   * é”€æ¯èµ„æºï¼ˆdisposeåˆ«åï¼‰
   */
  dispose(): void {
    this.cleanup();
  }

  /**
   * è·å–åª’ä½“æµ
   */
  getStream(): MediaStream | null {
    return this.stream;
  }

  /**
   * è®¾ç½®å¤–éƒ¨è§†é¢‘å…ƒç´ 
   */
  setExternalVideoElement(videoElement: HTMLVideoElement): void {
    this.videoElement = videoElement;
  }

  /**
   * è·å–å½“å‰çŠ¶æ€
   */
  getStatus() {
    return {
      isCapturing: this.isCapturing,
      hasVideo: this.stream?.getVideoTracks().length ?? 0 > 0,
      hasAudio: this.stream?.getAudioTracks().length ?? 0 > 0,
      audioContextState: this.audioContext?.state || 'suspended',
      faceChangeScore: this.faceChangeScore,
      webrtcConnectionState: 'connected' as RTCPeerConnectionState // æ·»åŠ ç¼ºå¤±çš„å±æ€§
    };
  }
}