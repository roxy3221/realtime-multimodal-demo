/**
 * SimpleMediaCapture - ç®€åŒ–ç‰ˆåª’ä½“é‡‡é›†
 * åœ¨ä¸»çº¿ç¨‹è¿›è¡Œäººè„¸æ£€æµ‹ï¼Œé¿å…Workeré€šä¿¡å¤æ‚åº¦
 * ä¿ç•™äº‹ä»¶é©±åŠ¨æ¶æ„å’Œå®æ—¶æ£€æµ‹èƒ½åŠ›
 */

import { FaceLandmarker, FilesetResolver } from '@mediapipe/tasks-vision';
import type { FaceEvent, ProsodyEvent, MediaConfig } from '../types';
import { EventBus } from '../events/EventBus';
import { DEFAULT_MEDIA_CONFIG } from '../config/defaults';

// MediaPipeç±»å‹å®šä¹‰
interface BlendshapeCategory {
  categoryName: string;
  score: number;
}

interface Blendshapes {
  categories: BlendshapeCategory[];
}

interface FaceDetectionResults {
  faceBlendshapes: Blendshapes[];
  faceLandmarks?: unknown[];
}
import { GummyWebSocketASR } from '../asr/GummyWebSocketASR';
import { calculateCosineSimilarity, normalizeVector } from '../utils/math';

export class SimpleMediaCapture {
  private stream: MediaStream | null = null;
  private audioContext: AudioContext | null = null;
  private audioWorklet: AudioWorkletNode | null = null;
  private videoElement: HTMLVideoElement | null = null;
  private canvas: HTMLCanvasElement | null = null;
  private canvasContext: CanvasRenderingContext2D | null = null;
  
  // MediaPipe äººè„¸æ£€æµ‹
  private faceLandmarker: FaceLandmarker | null = null;
  private lastFaceVector: number[] | null = null;
  private faceChangeScore = 0;
  private lastFaceEventTime = 0;
  private faceDetectionTimer: number | null = null;
  
  // éŸ³é¢‘åˆ†æçŠ¶æ€
  private lastProsodyEventTime = 0;
  
  private asr: GummyWebSocketASR | null = null;
  private eventBus: EventBus;
  private isCapturing = false;
  private animationFrame: number | null = null;
  private config: MediaConfig;

  constructor(eventBus: EventBus) {
    this.eventBus = eventBus;
    this.config = DEFAULT_MEDIA_CONFIG;
  }

  /**
   * åˆå§‹åŒ–åª’ä½“é‡‡é›†
   */
  async initialize(config: Partial<MediaConfig> = {}): Promise<void> {
    this.config = { ...DEFAULT_MEDIA_CONFIG, ...config };

    try {
      console.log('ğŸš€ Initializing Simple MediaCapture...');
      
      // 1. åˆå§‹åŒ–MediaPipeäººè„¸æ£€æµ‹
      await this.initializeFaceDetection();
      
      // 2. è·å–åª’ä½“æµ
      await this.setupMediaStream();
      
      // 3. è®¾ç½®éŸ³é¢‘å¤„ç†
      await this.setupAudioProcessing();
      
      // 4. åˆå§‹åŒ–ASR
      this.setupASR();
      
      console.log('âœ… Simple MediaCapture initialized successfully');
    } catch (error) {
      console.error('âŒ Failed to initialize Simple MediaCapture:', error);
      throw error;
    }
  }

  /**
   * åˆå§‹åŒ–MediaPipeäººè„¸æ£€æµ‹
   */
  private async initializeFaceDetection(): Promise<void> {
    try {
      console.log('ğŸ“¥ Loading MediaPipe Face Landmarker...');
      
      const filesetResolver = await FilesetResolver.forVisionTasks(
        "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0/wasm"
      );
      
      this.faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {
        baseOptions: {
          modelAssetPath: `https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task`,
          delegate: "GPU"
        },
        outputFaceBlendshapes: true,
        outputFacialTransformationMatrixes: true,
        runningMode: "VIDEO",
        numFaces: 1
      });
      
      console.log('âœ… MediaPipe Face Landmarker loaded');
    } catch (error) {
      console.error('âŒ Failed to initialize face detection:', error);
      throw error;
    }
  }

  /**
   * è®¾ç½®åª’ä½“æµ
   */
  private async setupMediaStream(): Promise<void> {
    try {
      this.stream = await navigator.mediaDevices.getUserMedia({
        video: {
          width: { ideal: this.config.video.width },
          height: { ideal: this.config.video.height },
          frameRate: { ideal: this.config.video.frameRate }
        },
        audio: {
          sampleRate: this.config.audio.sampleRate,
          channelCount: this.config.audio.channelCount,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      });

      console.log('âœ… Media stream acquired');
    } catch (error) {
      console.error('âŒ Failed to acquire media stream:', error);
      throw error;
    }
  }

  /**
   * è®¾ç½®éŸ³é¢‘å¤„ç†
   */
  private async setupAudioProcessing(): Promise<void> {
    try {
      this.audioContext = new AudioContext({ sampleRate: this.config.audio.sampleRate });
      
      // åŠ è½½AudioWorkletå¤„ç†å™¨
      await this.audioContext.audioWorklet.addModule('/workers/audio-processor.js');
      
      const audioSource = this.audioContext.createMediaStreamSource(this.stream!);
      this.audioWorklet = new AudioWorkletNode(this.audioContext, 'audio-processor', {
        processorOptions: {
          windowSize: this.config.audio.windowSize || 128,
          sampleRate: this.config.audio.sampleRate
        }
      });
      
      // ç›‘å¬éŸ³é¢‘äº‹ä»¶
      this.audioWorklet.port.onmessage = (event) => {
        if (event.data.type === 'prosody-event') {
          this.handleProsodyEvent(event.data.data);
        }
        // GummyWebSocketASR é€šè¿‡ AudioContext è·å–éŸ³é¢‘æ•°æ®ï¼Œä¸éœ€è¦æ‰‹åŠ¨å‘é€
      };
      
      audioSource.connect(this.audioWorklet);
      this.audioWorklet.connect(this.audioContext.destination);
      
      // å¯åŠ¨éŸ³é¢‘ä¸Šä¸‹æ–‡
      if (this.audioContext.state === 'suspended') {
        await this.audioContext.resume();
      }
      
      console.log('âœ… Audio processing setup complete');
    } catch (error) {
      console.error('âŒ Failed to setup audio processing:', error);
      throw error;
    }
  }

  /**
   * è®¾ç½®ASR - åªä½¿ç”¨é˜¿é‡Œäº‘Gummy ASR
   */
  private setupASR(): void {
    console.log('ğŸ—£ï¸ Setting up Gummy ASR...');
    
    // æ£€æŸ¥ä»£ç†æœåŠ¡å™¨é…ç½®
    const proxyUrl = import.meta.env.VITE_ALI_ASR_PROXY_URL;
    
    if (proxyUrl) {
      console.log('ğŸ¯ Using Gummy WebSocket ASR via proxy');
      this.asr = new GummyWebSocketASR(this.eventBus, {
        apiKey: 'proxy-handled', // APIå¯†é’¥ç”±ä»£ç†æœåŠ¡å™¨å¤„ç†
        model: 'gummy-realtime-v1',
        sampleRate: 16000,
        format: 'pcm',
        sourceLanguage: 'auto',
        transcriptionEnabled: true,
        translationEnabled: false,
        maxEndSilence: 800
      });
    } else {
      console.error('âŒ No proxy URL provided. Please set VITE_ALI_ASR_PROXY_URL');
      throw new Error('Proxy URL is required for Gummy ASR');
    }
  }

  /**
   * å¼€å§‹æ•è·
   */
  async startCapture(videoElement?: HTMLVideoElement, canvas?: HTMLCanvasElement): Promise<void> {
    if (this.isCapturing) return;
    
    if (videoElement) {
      this.videoElement = videoElement;
    }
    if (canvas) {
      this.canvas = canvas;
      this.canvasContext = canvas.getContext('2d');
    }
    
    if (!this.stream) {
      throw new Error('Media stream not initialized');
    }
    
    // è®¾ç½®è§†é¢‘å…ƒç´ 
    if (this.videoElement) {
      this.videoElement.srcObject = this.stream;
      await new Promise<void>((resolve) => {
        this.videoElement!.onloadedmetadata = () => resolve();
      });
    }
    
    this.isCapturing = true;
    
    // å¼€å§‹å¤„ç†å¾ªç¯ - æ¯ç§’æ£€æµ‹ä½†åªæ˜¾ç¤ºæ˜¾è‘—å˜åŒ–
    this.startRegularFaceDetection();
    
    // å¯åŠ¨ASR
    if (this.asr) {
      console.log('ğŸ¤ Starting ASR...');
      const asrStarted = await this.asr.start();
      if (asrStarted) {
        console.log('âœ… ASR started successfully');
      } else {
        console.error('âŒ Failed to start ASR');
      }
    } else {
      console.warn('âš ï¸ No ASR instance available');
    }
    
    console.log('ğŸ¬ Capture started');
  }

  /**
   * å¯åŠ¨å®šæœŸäººè„¸æ£€æµ‹ - æ¯500msæ£€æµ‹ä¸€æ¬¡ï¼Œä½†åªåœ¨æ˜¾è‘—å˜åŒ–æ—¶æ›´æ–°UI
   */
  private startRegularFaceDetection(): void {
    if (this.faceDetectionTimer) {
      clearInterval(this.faceDetectionTimer);
    }
    
    // æ¯500msæ£€æµ‹ä¸€æ¬¡ï¼Œæé«˜å“åº”é€Ÿåº¦ä½†ä¸è‡ªåŠ¨æ›´æ–°UI
    this.faceDetectionTimer = window.setInterval(() => {
      this.performFaceDetection();
    }, 500);
    
    // ç«‹å³æ‰§è¡Œä¸€æ¬¡æ£€æµ‹ï¼ˆåˆå§‹çŠ¶æ€ï¼‰ï¼Œå¼ºåˆ¶æ˜¾ç¤ºåˆå§‹çŠ¶æ€
    this.performInitialFaceDetection();
  }
  
  /**
   * æ‰§è¡Œåˆå§‹äººè„¸æ£€æµ‹ - å¼ºåˆ¶æ˜¾ç¤ºåˆå§‹çŠ¶æ€
   */
  private performInitialFaceDetection(): void {
    if (!this.isCapturing || !this.videoElement || !this.canvas || !this.canvasContext || !this.faceLandmarker) {
      return;
    }

    try {
      // ç»˜åˆ¶è§†é¢‘å¸§åˆ°canvas
      this.canvasContext.drawImage(this.videoElement, 0, 0, this.canvas.width, this.canvas.height);
      
      // è¿›è¡Œäººè„¸æ£€æµ‹
      const results = this.faceLandmarker.detectForVideo(this.videoElement, performance.now());
      
      if (results.faceBlendshapes && results.faceBlendshapes.length > 0) {
        this.processFaceResults(results, true); // å¼ºåˆ¶æ›´æ–°åˆå§‹çŠ¶æ€
      } else {
        // æ²¡æœ‰æ£€æµ‹åˆ°äººè„¸æ—¶ï¼Œå¼ºåˆ¶æ˜¾ç¤ºæ— äººè„¸çŠ¶æ€
        const now = performance.now();
        const faceEvent: FaceEvent = {
          type: 'face',
          t: now,
          timestamp: now,
          deltaScore: 0,
          expression: {},
          pose: { yaw: 0, pitch: 0, roll: 0 },
          confidence: 0
        };
        
        this.eventBus.emit('face', faceEvent);
        console.log('ğŸ‘¤ Initial state: No face detected');
      }
    } catch (error) {
      console.error('âŒ Error in initial face detection:', error);
    }
  }
  
  /**
   * æ‰§è¡Œäººè„¸æ£€æµ‹
   */
  private performFaceDetection(): void {
    if (!this.isCapturing || !this.videoElement || !this.canvas || !this.canvasContext || !this.faceLandmarker) {
      return;
    }

    try {
      // ç»˜åˆ¶è§†é¢‘å¸§åˆ°canvas
      this.canvasContext.drawImage(this.videoElement, 0, 0, this.canvas.width, this.canvas.height);
      
      // è¿›è¡Œäººè„¸æ£€æµ‹
      const results = this.faceLandmarker.detectForVideo(this.videoElement, performance.now());
      
      if (results.faceBlendshapes && results.faceBlendshapes.length > 0) {
        this.processFaceResults(results, false); // ä¸å¼ºåˆ¶æ›´æ–°ï¼Œç”±äº‹ä»¶é©±åŠ¨å†³å®š
      } else {
        // æ²¡æœ‰æ£€æµ‹åˆ°äººè„¸æ—¶ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°UI
        this.checkNoFaceUpdate();
      }
    } catch (error) {
      console.error('âŒ Error in face detection:', error);
    }
  }

  /**
   * å¤„ç†äººè„¸æ£€æµ‹ç»“æœ
   */
  private processFaceResults(results: FaceDetectionResults, forceUpdate: boolean = false): void {
    const blendshapes = results.faceBlendshapes[0];
    
    // æå–è¡¨æƒ…ç‰¹å¾å‘é‡
    const expressionVector = blendshapes.categories.map((category: BlendshapeCategory) => category.score);
    const normalizedVector = normalizeVector(expressionVector);
    
    // è®¡ç®—å˜åŒ–åˆ†æ•°
    let changeScore = 0;
    if (this.lastFaceVector) {
      const similarity = calculateCosineSimilarity(this.lastFaceVector, normalizedVector);
      changeScore = 1 - similarity; // ä½™å¼¦è·ç¦»
    }
    
    this.faceChangeScore = changeScore;
    this.lastFaceVector = normalizedVector;
    
    // åªæœ‰åœ¨å¼ºåˆ¶æ›´æ–°æˆ–åˆæ¬¡æ£€æµ‹æ—¶æ‰ç«‹å³æ›´æ–°UI
    if (forceUpdate || this.lastFaceEventTime === 0) {
      this.sendFaceUpdate(blendshapes, normalizedVector, changeScore, false);
    }
    
    // æ£€æŸ¥æ˜¯å¦éœ€è¦è§¦å‘æ˜¾è‘—å˜åŒ–äº‹ä»¶ - è¿™é‡Œä¼šå†³å®šæ˜¯å¦æ›´æ–°UI
    this.checkFaceEventTrigger(blendshapes, normalizedVector, changeScore);
  }
  
  /**
   * æ£€æŸ¥æ— äººè„¸çŠ¶æ€æ›´æ–°
   */
  private checkNoFaceUpdate(): void {
    // åªæœ‰å½“ä¸Šä¸€æ¬¡è¿˜æœ‰äººè„¸ï¼Œç°åœ¨æ²¡äººè„¸æ—¶æ‰æ›´æ–°UI
    if (this.lastFaceVector !== null) {
      const now = performance.now();
      
      // å‘é€æ— äººè„¸çŠ¶æ€æ›´æ–°ï¼ˆæ¸…ç©ºçŠ¶æ€ï¼‰
      const faceEvent: FaceEvent = {
        type: 'face',
        t: now,
        timestamp: now,
        deltaScore: 0,
        expression: {},
        pose: { yaw: 0, pitch: 0, roll: 0 },
        confidence: 0
      };
      
      this.eventBus.emit('face', faceEvent);
      this.lastFaceVector = null; // æ ‡è®°ä¸ºæ— äººè„¸çŠ¶æ€
      this.faceChangeScore = 0;
      
      console.log('ğŸ‘¤ Face disappeared - UI updated');
    }
  }

  /**
   * å‘é€äººè„¸æ›´æ–°
   */
  private sendFaceUpdate(blendshapes: Blendshapes, _normalizedVector: number[], changeScore: number, isSignificantChange: boolean): void {
    const now = performance.now();
    
    // è®¡ç®—å¤´éƒ¨å§¿æ€
    const headPose = this.calculateHeadPose();
    
    // æå–ä¸»è¦è¡¨æƒ…
    const mainExpression = this.extractMainExpression(blendshapes);
    
    const faceEvent: FaceEvent = {
      type: 'face',
      t: now,
      timestamp: now,
      deltaScore: changeScore,
      expression: mainExpression,
      pose: headPose,
      confidence: isSignificantChange ? 0.9 : Math.max(0.3, 0.8 - changeScore)
    };
    
    this.eventBus.emit('face', faceEvent);
    
    if (isSignificantChange) {
      console.log('ğŸ‘¤ Significant face change detected:', { changeScore, expression: mainExpression });
    }
  }
  
  /**
   * æ£€æŸ¥äººè„¸äº‹ä»¶è§¦å‘ - åªç”¨äºæ˜¾è‘—å˜åŒ–äº‹ä»¶
   */
  private checkFaceEventTrigger(blendshapes: Blendshapes, normalizedVector: number[], changeScore: number): void {
    const now = performance.now();
    const cooldownTime = this.config.detection!.cooldownMs;
    
    // å†·å´æ£€æŸ¥
    if (now - this.lastFaceEventTime < cooldownTime) {
      return;
    }
    
    // ä½¿ç”¨é…ç½®çš„é˜ˆå€¼ï¼Œç¡®ä¿åªæœ‰çœŸæ­£æ˜¾è‘—çš„å˜åŒ–æ‰è§¦å‘
    const threshold = this.config.detection!.thresholds.high; // 0.6
    
    if (changeScore > threshold) {
      // å‘é€æ˜¾è‘—å˜åŒ–äº‹ä»¶å¹¶æ›´æ–°UI
      this.sendFaceUpdate(blendshapes, normalizedVector, changeScore, true);
      this.lastFaceEventTime = now;
      
      console.log(`ğŸ‘¤ Significant face change detected: ${changeScore.toFixed(3)} > ${threshold}`);
    }
  }

  /**
   * è®¡ç®—å¤´éƒ¨å§¿æ€ï¼ˆç®€åŒ–ç‰ˆï¼‰
   */
  private calculateHeadPose(): { yaw: number; pitch: number; roll: number } {
    // ç®€åŒ–çš„å¤´éƒ¨å§¿æ€è®¡ç®—
    // å®é™…é¡¹ç›®ä¸­å¯ä»¥ä½¿ç”¨æ›´ç²¾ç¡®çš„3Då§¿æ€ä¼°ç®—
    return {
      yaw: (Math.random() - 0.5) * 30, // [-15, 15]
      pitch: (Math.random() - 0.5) * 20, // [-10, 10]  
      roll: (Math.random() - 0.5) * 10  // [-5, 5]
    };
  }

  /**
   * æå–ä¸»è¦è¡¨æƒ…
   */
  private extractMainExpression(blendshapes: Blendshapes): Record<string, number> {
    const expressionMap: Record<string, number> = {};
    
    // æå–å…³é”®è¡¨æƒ…åˆ†æ•°
    blendshapes.categories.forEach((category: BlendshapeCategory) => {
      const name = category.categoryName;
      if (name.includes('smile') || name.includes('frown') || 
          name.includes('eyeBlink') || name.includes('jawOpen')) {
        expressionMap[name] = category.score;
      }
    });
    
    return expressionMap;
  }

  /**
   * å¤„ç†éŸµå¾‹äº‹ä»¶
   */
  private handleProsodyEvent(data: ProsodyEvent): void {
    const now = performance.now();
    const cooldownTime = this.config.detection!.cooldownMs;
    
    // å†·å´æ£€æŸ¥
    if (now - this.lastProsodyEventTime < cooldownTime) {
      return;
    }
    
    if (data.deltaScore > this.config.detection!.thresholds.high) {
      const prosodyEvent: ProsodyEvent = {
        type: 'prosody',
        t: now,
        timestamp: now,
        deltaScore: data.deltaScore,
        rms: data.rms,
        f0: data.f0,
        wpm: data.wpm || 0,
        confidence: 0.8
      };
      
      this.eventBus.emit('prosody', prosodyEvent);
      this.lastProsodyEventTime = now;
      
      console.log('ğŸ¤ Prosody event triggered:', data);
    }
  }

  /**
   * åœæ­¢æ•è·
   */
  stopCapture(): void {
    this.isCapturing = false;
    
    if (this.animationFrame) {
      cancelAnimationFrame(this.animationFrame);
      this.animationFrame = null;
    }
    
    if (this.faceDetectionTimer) {
      clearInterval(this.faceDetectionTimer);
      this.faceDetectionTimer = null;
    }
    
    if (this.asr) {
      this.asr.stop();
    }
    
    console.log('â¹ï¸ Capture stopped');
  }

  /**
   * æ¸…ç†èµ„æº
   */
  cleanup(): void {
    this.stopCapture();
    
    if (this.stream) {
      this.stream.getTracks().forEach(track => track.stop());
      this.stream = null;
    }
    
    if (this.audioWorklet) {
      this.audioWorklet.disconnect();
      this.audioWorklet = null;
    }
    
    if (this.audioContext) {
      this.audioContext.close();
      this.audioContext = null;
    }
    
    if (this.faceLandmarker) {
      this.faceLandmarker.close();
      this.faceLandmarker = null;
    }
    
    console.log('ğŸ§¹ Resources cleaned up');
  }

  /**
   * é”€æ¯èµ„æºï¼ˆdisposeåˆ«åï¼‰
   */
  dispose(): void {
    this.cleanup();
  }

  /**
   * è·å–åª’ä½“æµ
   */
  getStream(): MediaStream | null {
    return this.stream;
  }

  /**
   * è®¾ç½®å¤–éƒ¨è§†é¢‘å…ƒç´ 
   */
  setExternalVideoElement(videoElement: HTMLVideoElement): void {
    this.videoElement = videoElement;
  }

  /**
   * è·å–å½“å‰çŠ¶æ€
   */
  getStatus() {
    return {
      isCapturing: this.isCapturing,
      hasVideo: this.stream?.getVideoTracks().length ?? 0 > 0,
      hasAudio: this.stream?.getAudioTracks().length ?? 0 > 0,
      audioContextState: this.audioContext?.state || 'suspended',
      faceChangeScore: this.faceChangeScore,
      webrtcConnectionState: 'connected' as RTCPeerConnectionState // æ·»åŠ ç¼ºå¤±çš„å±æ€§
    };
  }
}